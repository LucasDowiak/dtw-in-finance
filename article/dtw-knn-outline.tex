\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathbbol}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{pdflscape}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage[parfill]{parskip}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\graphicspath{{/Users/lucasdowiak/Git/k-similar-neighbor/article/images/}}

\usepackage[
  backend=biber,
  url=false,
  doi=true,
  style=authoryear,
  citestyle=apa,
  sorting=nty
]{biblatex}

\addbibresource{dtw_references.bib}
%\bibliography{dtw_references}

\begin{document}

\section{Introduction}

Measuring the association of two assets is a fundamental concept in finance and economics. Portfolio managers have to think about it when they open or close a position \cite{Markowitz1952PortfolioSelection, sharpe1963simplified}. Regulators and large Bank Holding Companies (BHC) have to consider it when designing stress tests. The most common method of measuring association is the Pearson correlation coefficient:

\begin{equation} \label{eq:pearsons_rho}
\rho(\boldsymbol{x}, \boldsymbol{y}) = \frac{ \sum_{t=1}^{T} (x_{t} - \bar{x}) (y_{t} - \bar{y})}{ \sqrt{\sum_{t=1}^{T} (x_{t} - \bar{x})^{2} \sum_{t=1}^{T} (y_{t} - \bar{y})^{2}}}
\end{equation}

There's certainly nothing wrong about this statistic as the measure of association. I use it all the time. And even though the economic literature is clear-eyed about it's shortcomings (asymmetry, fat-tails) when trying to capture true return correlations between different financial time series, it's still the first measure researches estimate. When looking at the covariance expression on the numerator of equation (\ref{eq:pearsons_rho}), the correlation's assessment of association is based on the co-movements of the two variables for each time unit. This tight pairing is completely logical but it can also be restrictive. For instance, if I were to sample T draws from the following multivariate normal distribution:

\begin{equation}
    \boldsymbol{Z_{t}} \thicksim N(\boldsymbol{0}, \boldsymbol{\Sigma})
\end{equation}

with 

\begin{equation}
    \boldsymbol{\Sigma} = \begin{bmatrix}
        \sigma_{1}^{2}             & \rho \sigma_{1}\sigma_{2} \\
        \rho \sigma_{1} \sigma_{2} & \sigma_{2}^{2} 
    \end{bmatrix}
\end{equation}

and I estimated the sample variance-covariance matrix, we should not be surprised to find that the sample statistics are close to the population values: $\sigma_{1} \approx \hat{s}_{1}$, $\sigma_{2} \approx \hat{s}_{2}$, and $\rho \approx \hat{\rho}$. Now, if we create a second multivariate sample $\boldsymbol{Z}^\prime = [\boldsymbol{Z}_{(-T), 1}, \, \boldsymbol{Z}_{(-1), 2}]$ where the first column is the all but the last value of $\boldsymbol{Z}_{1}$ while the second column is all but the first value of $\boldsymbol{Z}_{2}$. In this case we still have $\sigma^{\prime}_{1} \approx \hat{s}^{\prime}_{1}$ and $\sigma^{\prime}_{2} \approx \hat{s}^{\prime}_{2}$ but $\hat{\rho}^{\prime}$ will be a completely untrustworthy estimate of the original correlation because of the misalignment of the time index. This does not mean that the two variables are unrelated, they still are, but the relationship has been masked.

For this paper, I want to investigate other approaches for measuring the association of different time series and compare the results of those approaches to the standard correlation approach used in finance. My main point of reference has been an article by \cite{ElsingAgon2012} that explores topics in time series data mining. The authors provide a detailed survey of time series representations, various distance metrics for measures of association, and proper ways to index time series data for efficient querying and information retrieval. The discussions around various distance metrics are very interesting. \cite{ElsingAgon2012} categorize the these distance measures into four categories collected in figure \ref{fig:ds_dist_meas_table}. A very similar paper was published in Computational Economics very recently by \cite{FrancesWiemann2020}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{ts_distance_method.png}
    \caption{List of distance measures included in \cite{ElsingAgon2012}}
    \label{fig:ds_dist_meas_table}
  \end{figure}

Instead of focusing on exact synchronous co-movements, many of the distance measures discussed in the data mining literature focus on comparing the shape or global structure of two different series. Below are a handful of distance measures that are mentioned in figure \ref{fig:ds_dist_meas_table} that I believe can be easily implemented.

\begin{itemize}
    \item Pearson Correlation
\end{itemize}

\begin{itemize}
    \item Pearson Correlation
    \item $L_{p}$ norms
    \item Dynamic Time Warping
    \item Parameter value clustering (or by other metrics e.g. persistance)
    \item Autocorrelation
    \item Kullback-Leibler
\end{itemize}


\section{Relevant Literature}

\parencite{ElsingAgon2012} view the analysis of time series data from a data mining perspective. The authors provide a detailed survey of time series representations, various distance metrics for measuring association, and proper ways to index time series data for efficient querying and information retrieval. The distance measure are classified into four categories: shape, edit, feature, and structure-based.

\parencite{FruhwirthKaufmann2004} cluster time series into K groups based on the fitted parameters of AR(p) and dynamic regression models.

\parencite{Mueller2007} provides an excellent summary of the principles of DTW and discuss several extensions with respect to the local and global parameters of the technique.

\parencite{FrancesWiemann2020} apply dynamic time warping to quarterly real GDP of the 50 US states during the 2007 recession. The authors employ a novel distance metric and utlilize a K-Means algorithm proposed by \cite{PETITJEAN2011678} and adapted for time series to group the US states into seven distinct business cycle clusters.

\parencite{WangXieHanSun2012} apply DTW to measure the pair-wise similarity between 35 foreign currencies. The authors then utilize a minimum spanning trees (MST) -- a novel graph algorithm -- to document the structural changes in the dependencies of the FX market over three separate years.

\parencite{KotsifakosAthitsosPapapetrou_2011} are also motivitated from a data mining perspective and the ability to return relevant results from query to a large number of time series. The authors compare the an example-base query using DTW with a model-based approach using Hidden Markov Models (HMM).

\parencite{KeoghRatanamahatana_2005} Provide a theory for the most efficient lower bound on the dtw value. The authors adapt Piecewise Aggregate Approximation (PAA) for comparing series in a time-warping context.

\parencite{JEONG20112231} Instead of providing a universal bound or step condition the authors come up with a temporal weighting function and apply it to the cost matrix before calculating the accumulated cost matrix. For temporal weights the authors use a generalized logistic function that uses two parameters to center and scale the output of the function. The scale parameter is optimized on a validation set while the Performance is evaluated on a test set. 

\parencite{DuyTakeuchi2023statistical} Rare paper dealing directly with statistical inference of the DTW measure.

\parencite{WangXieHanSun2012} TBD

\parencite{HowardTalisAlexeev_2020} Unpublished manuscript attempt that applies a DTW step to a classic CAPM stock return analysis. The CAPM model states that an asset's return is governed by the following relationship:

\begin{equation}
    E[r_{j}] = r_{f} + \beta_{j} (E[r_{m}] - r_{f})
\end{equation}

Instead of running the above regression using  synchronous returns the authors first "align" the sequences using the DTW algorithm and then calculate the betas $\beta_{j} = Cov(r_{t_{l}}, r_{s_{l}}) / Var(r_{m})$. The authors note that after approaching the estimation of $\beta$ in this way

\section{Data}

Studying the companies that make up the Standard and Poor's 500 will provide a rich sample to look for comparisons. Three different time periods will be considered. The financial crisis of 2007-2008, the moderate economic period between 2015 and 2016, and the dramatic shut-down economy of 2019-2020. With a focus on the these three time periods, the market upheaval surrounding the financial crisis and Covid-19 serve as a natural experiment with the moderate-but-steady growth of 2015-2016 acting as a control. This is especially true of the Covid-19 timeframe. Some industries bounced back strongly (Technology) while others struggled to recover (Travel, Hospitality). These alternate reactions will ensure that there is enough heterogeneity in the "shape" of the stock values of these companies to make meaningful comparisons. Natural industry classifications will also provide a good reference point to any latent clustering that is discovered.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{SandP_missing_map_pptx.png}
    \caption{Missing map of the S\&P data set. The red horizontal bars demark the timeframes under consideration. Of the 505 tickers in the dataset, there are 423 with a full series stock values from 2007 onward, 480 from 2015 onward, and 494 from 2019 onward.}
    \label{fig:SandP_missing_map}
\end{figure}


The companies comprising the S\&P's Wikipedia page\footnote{https://en.wikipedia.org/wiki/List\_of\_S\&P\_500\_companies} as of February 2021 are included in this essay's analysis. Using an API made freely available by Alpha Advantage\footnote{https://www.alphavantage.co/}, daily historical series of the current companies in the index have been obtained. Dates range from Nov 1999 to Feb 2021. See Figure \ref{fig:SandP_missing_map} for a visual summary of the price history for the individual companies in the index.

\section{Distance Measures}

\subsection{ARMA-GARCH Benchmark} \label{sec:ARMAGARCH-benchmark}

To benchmark the competing measures, a procedure is designed to estimate a valid ARMA-GARCH model for the log returns of each member of the S\&P 500. After controlling for the conditional mean (ARMA) and the conditional variance (GARCH) of the stock's DGP, pair-wise correlation values are calculated on the standardized residuals for each stock. For model validation, I will follow the same process as in section 4.3 of \cite{DowiakTV-COP}. Each model is checked to see if it is well specified and that the model residuals are abiding by the independent and identical distribution assumptions.

Define the return as $X_{t} = \log \left(S_{t}\right) - \log\left(S_{t-1}\right)$, where $S_{t}$ is the spot price at time $t$. The conditional mean for the log-returns can be formulated by the following ARMA process:

\begin{equation} \label{eqn:marginalModel}
    x_{t} = \mu_{t} + \epsilon_{t}
\end{equation}

\begin{equation}
    \mu_{t} = \mu(\phi, \theta, x_{\{s:\, s < t\}}, \varepsilon_{\{s:\, s < t\}}) = \phi_{0} + \sum_{i=1}^{p} \phi_{i} x_{t-i} + \sum_{j=1}^{q} \theta_{j} \, \varepsilon_{t - j} + \varepsilon_{t}
\end{equation}

where $\varepsilon_{s}$ is an innovation term that satisfies $E[\varepsilon_{s}] = 0$ and $E[\varepsilon^{2}_{s}] = \sigma^{2}_{s}$. The conditional volatility process for the models under consideration can be generalized with the following formula. Functions \emph{A}, \emph{B}, \emph{C} are generic stand-ins that will differ across the various GARCH specifications.

\begin{equation}
    \sigma^{2}_{t} = \emph{C}(\varepsilon^{2}_{t-1}, \, \sigma^{2}_{t-1}) + \sum_{j=1}^{m} \alpha_{j} \emph{A}(\varepsilon^{2}_{t - j}) + \sum_{i=1}^{k} \beta_{i} \emph{B}(\sigma^{2}_{t - i})
\end{equation}

The conditional mean and variance are used to center and scale the innovation terms.

\begin{equation}
    z_{t}(\phi, \theta, \alpha, \beta) = \frac{x_{t} - \mu(x_{t-1}, \phi, \theta)}{\sigma(x_{t-1}, \alpha, \beta)}
\end{equation}

By collecting all the parameters in the conditional mean and variance equations into one vector $\Delta = [\phi, \theta, \alpha, \beta]$, the functional form for the error terms $f$ can be written as a product of $\Delta$, the chosen error distribution $g$, and any necessary shape parameters $\lambda$ of the distribution:

\begin{equation}
    f(x_{t} | \mu_{t}, \sigma^{2}_{t}, \lambda, \Delta) = \frac{1}{\sigma_{t}} g(z_{t} | \lambda, \Delta)
\end{equation}

The challenge for automating this process is two-fold. First, the size of the full model space is quite large. To list out all the dimensions that must be considered:

\begin{itemize}
    \item ARIMA model
        \begin{itemize}
            \item Constant term
            \item Autoregressive order (p)
            \item Moving average order (q)
            \item Order of integration (d)
            \item Seasonal autoregressive order (P)
            \item Seasonal moving average order (Q)
            \item Order of seasonal integration (D)
        \end{itemize}
        \item GARCH model
        \begin{itemize}
            \item Constant term
            \item Autoregressive order (m)
            \item Moving average order (k)
            \item GARCH specification
            \item Error distribution
        \end{itemize}

\end{itemize}

A full grid-search over these dimensions is time-consuming and impractical. To reduce the number of specifications in the model set, the fitting procedure takes the following divide-and-conquer approach.

\begin{enumerate}
    \item \textbf{Estimate the conditional mean independently of the variance model.} Leveraging the work done by Hyndman and Khandakar \cite{HyndmanKhandakar2008AutoArima}, a step-wise strategy is used to search through the ARIMA model space for the best fit, which is evaluated via the Akaike information criterion (AIC). This is accomplished by using the Forecast package \cite{RForecast} running in the R statistical language \cite{RBase}. The following decisions are made: 
    
    \begin{itemize}
        \item Always include a constant term
        \item Set the integration terms to zero: $d = D = 0$
    \end{itemize}
    
    \item \textbf{Estimate a set of GARCH models.} Set the conditional mean model to the specification found in step 1. Then iterate over every GARCH specification in the model set, re-estimating the combined ARMA and GARCH parameters at the same time for each conditional variance model. This is accomplished by using the rugarch package \cite{Rugarch}. The model set that is searched through considers the following dimensions:
    
    \begin{itemize}
        \item Always include a constant term
        \item ARCH specification: m = \{1, 2\}
        \item GARCH specification: k = \{1, 2\}
        \item Distributions: \emph{g} = \{Normal, Student-t, Skewed Student-t \cite{FernandezSteel1998}\}
        \item Model Specification: \emph{A}, \emph{B}, \emph{C} = \{Standard GARCH \cite{Bollerslev1986Garch}, gjr-GARCH \cite{GJR1993Garch}, Component GARCH \cite{EngleLee1993APA} \}
    \end{itemize}
    
    With these dimensions, a total of 36 volatility models are available to choose from.

    \item \textbf{Select the best model specification.} The fitted residuals of a model are checked against a battery of tests to confirm the independent and identical assumptions as well as to verify the correct distribution has been selected. The tests used include: (a) Moment LM tests to check for any remaining auto-correlation in the first four moments, (b) Kolmogorov-Smirnov test to check the residuals against the chosen theoretical distribution, (c) Hong and Li \cite{HongLi2005} non-parametric density test jointly for i.i.d and correct distribution specification, (d) Shapiro-Wilks \cite{ShapiroWilks1965} test for normality, and (e) Jarque-Bera \cite{JarqueBera1980} test for joint normality for skew and kurtosis.
    
    With these tests in hand, finding the best model reduces to selecting the GARCH specification that:
    
    \begin{itemize}
        \item Passes all five distributional tests
        \item Minimizes the Bayesian information criterion (BIC)
    \end{itemize}

    If no specifications pass all five tests, then the one that minimizes the BIC across the 36 candidate models is selected.

\end{enumerate}

The specification for the ARMA process is found first independently of the GARCH process. Once the AR(p) and MA(q) orders have been found, this specification is set and remains the same as different volatility models are estimated. Note that the parameter estimates are not held constant, just the specification. For each new GARCH fit, the ARMA parameters are all re-estimated.


\subsection{Dynamic Time Warping} \label{sec:DTW}

Dynamic time warping (DTW) is an alternative method for comparing the association between two discrete time series. It differs from Pearson's correlation measure in that the time indices between the two series at moments of comparison are not constrained to equal each other -- like in Equation (\ref{eq:pearsons_rho}). Time is allowed to stretch and compress before a local cost\footnote{This is the languaged used in \cite{Mueller2007} but other authors use "association", "dissimilarity", or "distance" functions.} $c$ function is applied to the pairs of values from the two time series. This article will adhere to the classic definition of dynamic time warping. For notation this article this article borrows heavily from \cite{Mueller2007}.

\subsubsection{Algorithm} \label{sec:DTW_Algorithm}

Suppose there are two time series: $x_{t}$ for $t \in [1:T]$ and $y_{s}$ for $s \in [1:S]$. A \emph{warping path} is a sequence $\boldsymbol{p} = [p_{1},..., p_{L}]$ where each element is a mapping from the time index of one series to the other: $p_{l} = (t_{l}, \, s_{l}) \in [1:T] \times [1:S]$ for $l \in [1:L]$. For each point in the warping path $p_{l}$ there is a cost function quantifying the distance between the values of the series.

\begin{equation}
    c: x_{t_{l}} \times y_{s_{l}} \rightarrow \mathbb{R}_{\ge 0}
\end{equation}

The behaviour of the warping paths do follow some conditions, which are listed below:

\begin{itemize}
    \item \emph{Boundary Condition}: $p_{1} = (1, 1)$ and $p_{L} = (T, S)$ 
    \item \emph{Step-Size Condition}: $p_{l} - p_{l - 1} \in \{ (1, 0), (0, 1), (1, 1) \} $
\end{itemize}

The boundary condition requires that the first and last indices of the two series are mapped to each other. The step-size condition governs the evolution of the warping path. It ensures a non-decreasing monotonicity in the indices of \emph{both} series such that $t_{i} \le t_{j}$ and $s_{i} \le s_{j}$ if $i \le j$. A $T \times S$ cost matrix can be created that stores the associated cost between all values in the two series:

\begin{equation}
    \mathbf{C}(\boldsymbol{X}, \boldsymbol{Y}) = \left[ 
        \begin{array}{cccc}
            c(x_{T}, y_{1}) & c(x_{T}, y_{2}) & \cdots & c(x_{T}, y_{S}) \\ 
            \vdots          & \vdots          & \vdots & \vdots          \\
            c(x_{2}, y_{1}) & c(x_{2}, y_{2}) & \cdots & c(x_{2}, y_{S}) \\ 
            c(x_{1}, y_{1}) & c(x_{1}, y_{2}) & \cdots & c(x_{1}, y_{S})
    \end{array}\right]
\end{equation}

A warping path's total cost is the sum of the local costs it incurs as it travels from the start of the series (bottom left of $\boldsymbol{C}$) to their end (top right of $\boldsymbol{C}$):

\begin{equation}
    \mathbb{c}_{\boldsymbol{p}}(\boldsymbol{X}, \boldsymbol{Y}) = \sum^{L}_{l=1} c(x_{t_{l}},\, y_{s_{l}})
\end{equation}

There are many permissable\footnote{Permissable as governed by the boundary and step conditions} warping paths between two series. In fact the number of permissable paths has exponential growth in the values of $T$ and $S$. The aim during optimization is to find the warping path that minimizes the total cost. If the set of all warping paths are denoted $\mathbb{P}(\boldsymbol{X}, \boldsymbol{Y})$ then the value of the optimal warping path has the property

\begin{equation}
    \mathbb{c}_{\boldsymbol{p}^{*}}(\boldsymbol{X}, \boldsymbol{Y}) \le \mathbb{c}_{\boldsymbol{p}}(\boldsymbol{X}, \boldsymbol{Y}) \,\, \textrm{for all} \,\, \boldsymbol{p} \in \mathbb{P}(\boldsymbol{X}, \boldsymbol{Y})
\end{equation}

and the value of the DTW measure between $\boldsymbol{X}$ and $\boldsymbol{Y}$ is set to $\mathbb{c}_{\boldsymbol{p^{*}}}(\boldsymbol{X}, \boldsymbol{Y})$. An interested party could solve this optimization problem by estimating the total cost of all warping paths and select the one that minimizes this value. The challenge though is efficient computation. Since the number of warping paths grows exponentially the compuational time needed to check every warping path becomes problematic for large series. The proposed solution is to leverage \emph{dynamic programming} to reduce the computational time needed to find the optimal solution. Instead of dealing with exponential growth of permissable warping paths the DTW algorithm can find the optimal warping path in $\mathcal{O}(TS)$ calculations. To do so an \emph{accumulated cost matrix} $\boldsymbol{A}$ needs to be defined. The accumulated cost matrix has the same dimension as the cost matrix. Defining $A_{t,s}$ as the value of $\boldsymbol{A}$ at the $t^{th}$ row and the $s^{th}$ column of the accumulated cost matrix, the matrix has the following three identities:

\begin{equation}
    A_{t,1} = \sum^{t}_{k=1} c(x_{k}, y_{1}) \,\, \textrm{for} \,\, t \in [1:T]
\end{equation}

\begin{equation}
    A_{1,s} = \sum^{s}_{k=1} c(x_{1}, y_{k}) \,\, \textrm{for} \,\, s \in [1:S]
\end{equation}

\begin{equation}
    A_{t, s} = c(x_{t}, y_{s}) + \min\left(A_{t-1, s-1}, \, A_{t-1, s}, \, A_{t, s-1}\right)
\end{equation}

With this definition of the accumulative cost matrix the optimal warping path can be found by the following recursive procedure:

\begin{enumerate}
    \item Set $p_{L} = (T, S)$
    \item Given $p_{l} = (t, s)$ select $p_{l - 1} = \begin{cases} (1, s-1) & \textrm{if } t=1 \\ (t-1,1) & \textrm{if } s=1 \\ 
                                            \argmin \left[ A_{t-1, s-1},\, A_{t, s-1},\, A_{t-1, s} \right] & \textrm{otherwise} \end{cases}$
\end{enumerate}

\subsubsection{Common Extensions}

The definition of the DTW in section \ref{sec:DTW_Algorithm} is an unconstrained one without restrictions beyond the boundary and step-size conditions. Most applications of dynamic time-warping constrain the extent of time distoration in some way. From a finance and macroeconomic perspective there are reasonable arguments for a limit as well. New information in markets dissipates over time. There are numerous modifications of the original DTW diverse set of industries and domains but applications to financial or economics are relatively sparse. Unit root behavior is a common challenge in the finance and macro-economic fields.

\begin{itemize}
    \item Temporal weight approaches
        \begin{itemize}
            \item \parencite{JEONG20112231}
            \item This article if we choose a weighting function suitable for 
        \end{itemize}
    \item Step-Size Constraints
        \begin{itemize}
            \item Sakoe and Chiba (1978)
        \end{itemize}
    \item Global Bounds
        \begin{itemize}
            \item Sakoe-Chiba Band, Itakura Parallelogram
        \end{itemize}
\end{itemize}

\subsubsection{Example}

To put these concepts into practice the dynamic time warping algorithm will be demonstrated on the stock prices for Agilent Technologies (A) and General Electric (GE) from the first half of 2019. Instead of the original price level the series will be standardized by setting the price on January 2, 2019 equal to one. Figure \ref{fig:dtw_plot_stock_price_index_matching} summarizes the price movement of these two stocks and shows which price values are matched together by their optimal warping path. Figure \ref{fig:dtw_plot_warping_between_agilent_and_ge} displays the full optimal warping paths and discusses the influence of boundary and step-size conditions on it's shape. Figure \ref{fig:dtw_plot_cost_and_accum_cost} displays the cost and accumulated cost matrices for the first ten days of the two series. The optimal warping path is annotated in both.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dtw_plot_stock_price_index_matching.jpeg}
    \caption{This figures shows a normalized view of the Agilent and GE stock price for the first half of 2019. The normalization divides the stock price by it's value on the first market day of the year, January 2nd, 2019. Dynamic time warping is applied on the standardized series. The total length is 124 days ending on June 28th. The dashed gray lines connect elements of the index pair in the optimized warping path. The DTW finds the warping path that has the least total cost.}
    \label{fig:dtw_plot_stock_price_index_matching}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dtw_plot_warping_between_agilent_and_ge.jpeg}
    \caption{This figure displays the full warping path between Agilent and GE stock prices in the first half of 2019. The two peripheral images show their respective standardized stock prices while the central image traces the warping path as it connects the indicies of the two series. The influence of the step-condition is seen immediately in the marginal increments of the path. They are non-decreasing. This allows time to "stretch" but not be put into reverse. At the end of the warping path (top right part of the image) the boundary condition becomes binding--seen by the vertical line at the tail end--as the last index in GE's series is reached before the last index of Agilent's.}
    \label{fig:dtw_plot_warping_between_agilent_and_ge}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dtw_plot_cost_and_accum_cost.jpeg}
    \caption{The left image is the cost matrix for the first ten values of the Agilent and GE stock prices after standardization. The local cost function is the euclidean distance. The right image is the accumulative cost matrix for the same sequence of prices. The optimal warping path is annotated with white outlines. The gray shading tracks the range of values in each matrix. Darker shades of gray represent smaller values while lighter shades of gray represents larger values. Note that the scale of shading is not uniform across images.}
    \label{fig:dtw_plot_cost_and_accum_cost}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dtw_plot_cost_matrix_topography.jpeg}
    \caption{This figure displays the optimal alignment between Aligent and GE stock return series. The topology lines are based on the cost matrix. From this view the warping path (in blue) is a "valley of least cost" traversing the local cost landscape.}
    \label{fig:dtw_plot_cost_matrix_topography}
\end{figure}


\pagebreak

\section{Pair Trading}

One way to compare these measures is putting them to practical use. In this section a simple trading strategy is described that centers around finding "similar" pairs of stocks and trading off the expectation that any major short term deviations in their prices are eventually unwound over a longer time horizon. The spcecifics of this pair trading strategy are the same as the approach taken by \cite{Gatev_et_al_2006}, with a few adjustments made to take into account the unique nautre of dynamic time warping and its difference to correlation as a measure of similarity. This backtesting experiment is not carried out because of high expected returns, especially in the modern trading environment. The rise and refinement of high frequency trading now sees arbitrage opportunities resolved in microseconds\footcite[See][]{Aquilina_et_al_2021} while this article studies daily asset returns. In addition \cite{Gatev_et_al_2006} was published close to two decades ago in a widely read journal making its recipe part of the established financial canon. The benefits to using this particular trading strategy is that it provides a suitable context to directly compare the impact of using dynamic time warping -- instead of correlation -- to identify the "similar" pairs to trade.

\begin{enumerate}
    \item During the 
    \item Pair stocks together that are highly associated with each other.
    \item Take advantage of any large temporary cumulative return deviations and bet on a likely re-alignment of the cumulative returns in the near future
    \item Close out any open positions at the end of the trading window
\end{enumerate}


There are three different propsed methods for chosing the association measure:
\begin{enumerate}
    \item Use the un-treated returns. Measure closeness via correlation. Use static (unconditional) distributions and a two standard deviation threshold on the cumulative return differential as a signal to open positions.
    \item Filter the returns through an ARIMA-GARCH model. Measure closeness on the fitted residuals. Use rolling mean/variance values to calculate adjusted threshold during trading period
    \item Calculate the DTW metric between the cumulative returns of the pairs. Use synchronous L1 value and the two standard deviation signal to open positions.
\end{enumerate}

TODO:
\begin{enumerate}
    \item Need to compare pairs formation between the three buy strategies
    \item Compare Top 5 and Top 20 correlation pairs. Returns between the two strategies are highly correlated. How often does the model make different pair recommendations?
\end{enumerate}

\begin{table}[!htb]
    \begin{center}
      \begin{tabular}{l r r r r r r r r}
        \multicolumn{3}{l}{I. Returns on Committed Equity} \\
        \hline
        \# Pairs & Pair Strategy & Mean & SD & Median & Skew & Kurtosis & Min & Max \\
        \hline
        Top 5   & unadjusted cor &  0.016 & 0.034 &  0.012 & -1.077 & 5.025 & -0.088 & 0.068 \\
        Top 5   &      model cor &  0.003 & 0.074 &  0.010 & -1.957 & 7.576 & -0.245 & 0.105 \\
        Top 5   &            dtw &  0.016 & 0.057 &  0.023 & -0.208 & 3.579 & -0.126 & 0.137 \\
        Top 20  & unadjusted cor & -0.001 & 0.036 &  0.004 & -2.079 & 7.794 & -0.123 & 0.047 \\
        Top 20  &      model cor &  0.003 & 0.031 &  0.007 & -1.793 & 7.498 & -0.103 & 0.054 \\
        Top 20  &            dtw &  0.016 & 0.042 &  0.014 &  0.159 & 3.232 & -0.072 & 0.100 \\
        101-120 & unadjusted cor & -0.002 & 0.034 & -0.004 & -1.070 & 5.818 & -0.110 & 0.052 \\
        101-120 &      model cor & -0.010 & 0.046 & -0.007 & -1.829 & 6.967 & -0.167 & 0.035 \\
        101-120 &            dtw &  0.005 & 0.068 &  0.002 &  0.193 & 2.870 & -0.125 & 0.155 \\
        \vspace{1 mm} \\
        \multicolumn{3}{l}{II. Returns on Fully Invested Equity} \\
        \hline
        \# Pairs & Pair Strategy & Mean & SD & Median & Skew & Kurtosis & Min & Max \\
        \hline
        Top 5   & unadjusted cor &  0.027 & 0.059 & 0.030  & -0.638 & 3.216 & -0.110 & 0.115 \\
        Top 5   &      model cor &  0.020 & 0.094 & 0.017  & -0.948 & 4.480 & -0.245 & 0.174 \\
        Top 5   &            dtw &  0.016 & 0.057 & 0.023  & -0.205 & 3.446 & -0.126 & 0.137 \\
        Top 20  & unadjusted cor &  0.008 & 0.055 & 0.006  & -0.686 & 5.218 & -0.145 & 0.132 \\
        Top 20  &      model cor &  0.012 & 0.051 & 0.013  & -0.686 & 5.285 & -0.138 & 0.117 \\
        Top 20  &            dtw &  0.016 & 0.043 & 0.014  &  0.184 & 3.151 & -0.072 & 0.100 \\
        101-120 & unadjusted cor & -0.005 & 0.059 & -0.006 & -1.053 & 5.217 & -0.183 & 0.085 \\
        101-120 &      model cor & -0.015 & 0.077 & -0.010 & -1.123 & 3.909 & -0.214 & 0.092 \\
        101-120 &            dtw &  0.005 & 0.069 &  0.003 &  0.163 & 2.855 & -0.125 & 0.155 \\
        \hline
      \end{tabular}
    \caption{Average Annual Return Distribution}
    \end{center}
    \begin{tablenotes}
        \item{\footnotesize Two different measures of portfolio returns are summarized in this table. The top section records committed returns where the total return of the portfolio during the trading period is divided by the number of pairs selected to trade. See column one in the table. Section II. of the table records fully invested returns where the the total return of the portfolio during the trading period is devided by the number of pairs selected to trade.}
    \end{tablenotes}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Annual_Returns_to_Committed_Capital.png}
    \caption{Barcharts indicating the annual return of the trading strategy under different buy signals and portfolio sizes.}
    \label{fig:annual_returns_to_committed_capital_by_buy_signal}
\end{figure}

\section{Simulation Results}
In order to gauge the relationship between the DGP process of two time series and their resulting DTW value we turn to simulation. For the first simulation a stationary process is developed with only two factors: an intercept and an error term.

\begin{equation}
    X_{t} = \omega + \epsilon_{t} \qquad \epsilon_{t} \thicksim \mathcal{N}(0, \sigma^{2})
\end{equation}

This stationary process represents a simplified ARIMA-GARCH model, replacing the conditional mean and variance with their unconditional equivalents. After fixing parameter values ($\omega_{A}$, $\sigma_{A}$, $\omega_{B}$, $\sigma_{B}$) we can sample from the normal distribution to produce stationary time series of arbitrary length. By sampling a series each from specifications A and B, then calculating the dynamic time-warping distance between the two, we can trace the effect of the parameters on the value of the DTW metric. By repeating this process N times we can build up a distribution of these values. A similar apprach can be used to capture the null distribution of the dynamic time-warping metric for two unit root processes that obey the following specification.

\begin{equation} \label{eq:random_walk}
    X_{t} = \omega + X_{t-1} + \epsilon_{t} \qquad \epsilon_{t} \thicksim \mathcal{N}(0, \sigma^{2})
\end{equation}

From equation (\ref{eq:random_walk}) at any time $t$ we know both the expectation and the variance of both random walk processes:

\begin{equation}
    X_{i,t} \thicksim \mathcal{N}(t\omega_{i}, \, t\sigma_{i}^{2})
\end{equation}

A random walk temporal weighting function could have the following form:

\begin{equation}
    \omega_{t, s} = \lvert t - s \rvert \sqrt{ \sigma_{A,\, t} \sigma_{B,\, s} }
\end{equation}

\begin{equation}
    \omega_{t, s} = \rho \lvert t - s \rvert \sigma_{A,\, t} \sigma_{B,\, s}
\end{equation}

\begin{equation}
    \omega(t, s) = \sum_{k=1}^{|t-s|} \beta^{|t-s|} \left( \sigma^{2}_{A,\, t} + \sigma^{2}_{B,\,s} \right)
\end{equation}

\begin{table}[!htb]
    \begin{center}
      \begin{tabular}{| r r r r | r r r r r r r |}
        \hline
        $\omega_{A}$ & $\omega_{B}$ & $\sigma^{2}_{A}$ & $\sigma^{2}_{B}$ & Min & 1st Qu. & Median & Mean & 3rd Qu. & Max & IQR \\
        \hline
        0  & 0 & 1  & 1  & 0.72  & 0.82  & 0.85  & 0.85  & 0.89  & 0.97  & 0.06 \\
        \hline
        0  & 0 & 1  & 2  & 0.86  & 1.04  & 1.08  & 1.09  & 1.13  & 1.25  & 0.09 \\
        0  & 0 & 1  & 4  & 1.19  & 1.44  & 1.52  & 1.52  & 1.59  & 1.83  & 0.15 \\
        0  & 0 & 1  & 8  & 1.72  & 2.07  & 2.18  & 2.19  & 2.29  & 2.70  & 0.22 \\
        0  & 0 & 1  & 16 & 2.55  & 2.99  & 3.16  & 3.17  & 3.32  & 4.05  & 0.33 \\
        \hline
        0  & 0 & 2  & 2  & 0.99  & 1.16  & 1.22  & 1.21  & 1.26  & 1.42  & 0.10 \\
        0  & 0 & 4  & 4  & 1.39  & 1.64  & 1.71  & 1.71  & 1.77  & 2.02  & 0.13 \\
        0  & 0 & 8  & 8  & 2.06  & 2.32  & 2.42  & 2.42  & 2.51  & 2.93  & 0.18 \\
        0  & 0 & 16 & 16 & 2.95  & 3.31  & 3.44  & 3.44  & 3.56  & 4.14  & 0.25 \\
        \hline
        2  & 0 & 1  & 1  &  1.23 &  1.41 &  1.48 &  1.48 &  1.53 &  1.80 & 0.12 \\
        4  & 0 & 1  & 1  &  2.32 &  3.32 &  3.58 &  3.58 &  3.86 &  4.76 & 0.54 \\
        8  & 0 & 1  & 1  &  9.47 & 11.06 & 11.41 & 11.39 & 11.75 & 12.86 & 0.69 \\
        16 & 0 & 1  & 1  & 25.16 & 27.06 & 27.41 & 27.38 & 27.74 & 28.68 & 0.68 \\
        \hline
      \end{tabular}
    \caption{Stationary White Noise}
    \end{center}
    \begin{tablenotes}
        \item{\footnotesize N = 500; T = 100}
        \item {\footnotesize \textbf{Note}: The DTW value is divided by the series length (T).}
        \item {\footnotesize \textbf{Note}: The distributions of the DTW samples are really symmetric. The median and mean remain close in value over all configurations.}
        \item{\footnotesize \textbf{Note}: For the stationary series the deterministic component (distance between the mean values) plays an influencial role in the location of the center of the DTW distribution.}
    \end{tablenotes}
\end{table}

\begin{table}[!htb]
    \begin{center}
      \begin{tabular}{| r r r r | r r r r r r r |}
        \hline
        $\omega_{A}$ & $\omega_{B}$ & $\sigma^{2}_{A}$ & $\sigma^{2}_{B}$ & Min & 1st Qu. & Median & Mean & 3rd Qu. & Max & IQR \\
        \hline
        0 & 0 & 1  & 1  & 0.84 &  2.36 &  4.42 &  7.17 &  9.31 &  46.87 &  6.94  \\
        \hline
        0 & 0 & 1  & 2  & 1.39 &  3.57 &  7.05 & 10.09 & 13.84 &  50.89 & 10.27  \\
        0 & 0 & 1  & 4  & 1.70 &  4.87 &  9.01 & 12.88 & 17.17 &  74.00 & 12.29  \\
        0 & 0 & 1  & 8  & 2.88 &  6.81 & 12.30 & 16.34 & 22.30 &  83.36 & 15.48  \\
        0 & 0 & 1  & 16 & 3.97 & 10.93 & 18.41 & 23.23 & 32.04 & 101.38 & 21.11  \\
        \hline
        0 & 0 & 2  & 2  & 1.43 &  3.44 &  6.48 & 10.41 & 13.85 &  69.20 & 10.41  \\
        0 & 0 & 4  & 4  & 1.73 &  4.83 &  8.98 & 14.41 & 19.41 &  91.95 & 14.58  \\
        0 & 0 & 8  & 8  & 2.75 &  7.38 & 15.66 & 23.12 & 31.83 & 152.90 & 24.45  \\
        0 & 0 & 16 & 16 & 4.26 & 10.05 & 19.77 & 31.23 & 40.61 & 251.30 & 30.56  \\
        \hline
      \end{tabular}
    \caption{Random Walk without Drift}
    \end{center}
    \begin{tablenotes}
        \item{\footnotesize N = 500; T = 100}
        \item {\footnotesize \textbf{Note}: The DTW value is divided by the series length (T).}
        \item{\footnotesize \textbf{Note}: }
    \end{tablenotes}
\end{table}

\begin{table}[!htb]
    \begin{center}
      \begin{tabular}{| r r r r | r r r r r r r |}
        \hline
        $\omega_{A}$ & $\omega_{B}$ & $\sigma^{2}_{A}$ & $\sigma^{2}_{B}$ & Min & 1st Qu. & Median & Mean & 3rd Qu. & Max & IQR \\
        \hline
        1  & 1 & 1  & 1  & 0.85   & 1.12   & 1.48   &  1.97  &  2.36  &  8.62  & 1.24  \\
        \hline
        1  & 1 & 1  & 2  & 1.00   & 1.42   & 2.02   &  2.75  &  3.48  & 12.82  & 2.05  \\
        1  & 1 & 1  & 4  & 1.31   & 1.97   & 2.79   &  4.01  &  4.62  & 19.91  & 2.64  \\
        1  & 1 & 1  & 8  & 1.68   & 3.08   & 4.55   &  6.49  &  8.08  & 40.11  & 5.00  \\
        1  & 1 & 1  & 16 & 2.88   & 5.19   & 7.64   & 11.02  & 13.26  & 68.12  & 8.07  \\
        \hline
        1  & 1 & 2  & 2  & 1.23   & 1.63   &  2.38  &  3.23  &  3.76  &  22.63 &  2.13  \\
        1  & 1 & 4  & 4  & 1.73   & 2.43   &  3.66  &  5.62  &  7.60  &  36.24 &  5.17  \\
        1  & 1 & 8  & 8  & 2.60   & 4.06   &  6.49  &  9.38  & 11.70  &  48.24 &  7.64  \\
        1  & 1 & 16 & 16 & 3.93   & 6.57   & 10.87  & 17.00  & 20.03  & 108.67 & 13.46  \\
        \hline
        2  & 1 & 1  & 1  &  15.18 &  27.33 &  32.43 &  32.74 &  37.32 &  59.31 &  9.99  \\
        4  & 1 & 1  & 1  & 115.79 & 136.75 & 143.33 & 143.48 & 150.31 & 181.75 & 13.56  \\
        8  & 1 & 1  & 1  & 355.60 & 382.08 & 389.51 & 389.48 & 396.75 & 422.52 & 14.67  \\
        16 & 1 & 1  & 1  & 854.08 & 883.86 & 891.20 & 891.33 & 899.22 & 927.54 & 15.36  \\
        \hline
      \end{tabular}
    \caption{Random Walk with Drift}
    \end{center}
    \begin{tablenotes}
        \item{\footnotesize N = 500; T = 100}
        \item {\footnotesize \textbf{Note}: The DTW value is divided by the series length (T).}
        \item {\footnotesize \textbf{Note}: The distributions are skewed towards larger values of the DTW metric (mean is greater than the median).}
        \item{\footnotesize \textbf{Note}: Compare the last row of each section where variance or the distance between intercepts is set to 16. For the white noise process the strongest factor contributing to the mean DTW value is the difference between the mean values. In comparison, the response to an increase in the variance of either series is muted.}
    \end{tablenotes}
\end{table}

\pagebreak

\printbibliography

\end{document}