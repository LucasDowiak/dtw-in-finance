\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathbbol}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{pdflscape}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage[parfill]{parskip}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\graphicspath{{/Users/lucasdowiak/Git/dtw-in-finance/article/images/}}

\usepackage[
  backend=biber,
  url=false,
  doi=true,
  style=numeric,
  citestyle=numeric,
  sorting=nty
]{biblatex}

\addbibresource{dtw_references.bib}
%\bibliography{dtw_references}

\begin{document}

\section{Introduction}

Measuring the association of two assets is a fundamental concept in finance and economics. Portfolio managers have to think about it when they open or close a position \cite{Markowitz1952PortfolioSelection, sharpe1963simplified}. Regulators and large Bank Holding Companies (BHC) have to consider it when designing stress tests. The most common method of measuring association is the Pearson correlation coefficient:

\begin{equation} \label{eq:pearsons_rho}
\rho(\boldsymbol{x}, \boldsymbol{y}) = \frac{ \sum_{t=1}^{T} (x_{t} - \bar{x}) (y_{t} - \bar{y})}{ \sqrt{\sum_{t=1}^{T} (x_{t} - \bar{x})^{2} \sum_{t=1}^{T} (y_{t} - \bar{y})^{2}}}
\end{equation}

There's certainly nothing wrong about this statistic as the measure of association. I use it all the time. And even though the economic literature is clear-eyed about it's shortcomings (asymmetry, fat-tails) when trying to capture true return correlations between different financial time series, it's still the first measure researches estimate. When looking at the covariance expression on the numerator of equation (\ref{eq:pearsons_rho}), the correlation's assessment of association is based on the co-movements of the two variables for each time unit. This tight pairing is completely logical but it can also be restrictive. For instance, if I were to sample T draws from the following multivariate normal distribution:

\begin{equation}
    \boldsymbol{Z_{t}} \thicksim N(\boldsymbol{0}, \boldsymbol{\Sigma})
\end{equation}

with 

\begin{equation}
    \boldsymbol{\Sigma} = \begin{bmatrix}
        \sigma_{1}^{2}             & \rho \sigma_{1}\sigma_{2} \\
        \rho \sigma_{1} \sigma_{2} & \sigma_{2}^{2} 
    \end{bmatrix}
\end{equation}

and I estimated the sample variance-covariance matrix, we should not be surprised to find that the sample statistics are close to the population values: $\sigma_{1} \approx \hat{s}_{1}$, $\sigma_{2} \approx \hat{s}_{2}$, and $\rho \approx \hat{\rho}$. Now, if we create a second multivariate sample $\boldsymbol{Z}^\prime = [\boldsymbol{Z}_{(-T), 1}, \, \boldsymbol{Z}_{(-1), 2}]$ where the first column is the all but the last value of $\boldsymbol{Z}_{1}$ while the second column is all but the first value of $\boldsymbol{Z}_{2}$. In this case we still have $\sigma^{\prime}_{1} \approx \hat{s}^{\prime}_{1}$ and $\sigma^{\prime}_{2} \approx \hat{s}^{\prime}_{2}$ but $\hat{\rho}^{\prime}$ will be a completely untrustworthy estimate of the original correlation because of the misalignment of the time index. This does not mean that the two variables are unrelated, they still are, but the relationship has been masked.

For this paper, I want to investigate other approaches for measuring the association of different time series and compare the results of those approaches to the standard correlation approach used in finance. My main point of reference has been an article by \cite{ElsingAgon2012} that explores topics in time series data mining. The authors provide a detailed survey of time series representations, various distance metrics for measures of association, and proper ways to index time series data for efficient querying and information retrieval. The discussions around various distance metrics are very interesting. \cite{ElsingAgon2012} categorize the these distance measures into four categories collected in figure \ref{fig:ds_dist_meas_table}. A very similar paper was published in Computational Economics very recently by \cite{FrancesWiemann2020}.

Instead of focusing on exact synchronous co-movements, many of the distance measures discussed in the data mining literature focus on comparing the shape or global structure of two different series. Below are a handful of distance measures that are mentioned in figure \ref{fig:ds_dist_meas_table} that I believe can be easily implemented.

\begin{itemize}
    \item Pearson Correlation
\end{itemize}

\begin{itemize}
    \item Pearson Correlation
    \item $L_{p}$ norms
    \item Dynamic Time Warping
    \item Parameter value clustering (or by other metrics e.g. persistance)
    \item Autocorrelation
    \item Kullback-Leibler
\end{itemize}


\section{Relevant Literature}

\cite{SakoeChiba_IEEE_1978} provides the original paper defining dynamic time warping in the field of speech recognition. It's influence has extended into applications such data mining, agricultural and forestry management, high-frequency trading.
Current applications in the finance and economic fields usually fall into one of three basic perspectives. 
\begin{enumerate}
    \item Latent cluster identification where no explicit labels are provided or prior knowledge of any underlying multinomial distribution
    \begin{itemize}
        \item \parencite{FruhwirthKaufmann2004} cluster time series into K groups based on the fitted parameters of AR(p) and dynamic regression models.
        \item \parencite{FrancesWiemann2020} apply dynamic time warping to quarterly real GDP of the 50 US states during the 2007 recession. The authors employ a novel distance metric and utlilize a K-Means algorithm proposed by \cite{PETITJEAN2011678} and adapted for time series to group the US states into seven distinct business cycle clusters.
        \item \cite{PETITJEAN2011678} illustrates a method to find a global average for a set of sequences. The "average" of a set of sequences is defined as the sequence that minimizes the within group sum of sequares distances from each sequence in the set to the "average" sequence. The distance in between sequences is calculated by the dynamic time warping algorithm. The bulk of the paper discusses the process of updating the "average" sequence from one iteration to the next. 
    \end{itemize}
    \item Template Pattern Matching and Nearest Neighbor Classification
    \begin{itemize}
        \item \cite{WAN2017151} uses DTW in support of technical anaylsis of financial assets.
        \item \cite{Petitjean_et_al_2016} demonstrates that \cite{PETITJEAN2011678} can be used to form efficient nearest neighbor classification decisions where the nearest group average determines the classification. Performs better that Euclidean averages or mediod 
    \end{itemize}
    \item Lead/Lag estimation. Temporal alignment. Linking asynchroneous time series. High frequency trading application. Leading economic indicator analysis. Land-use identification from Satelite data.
    \begin{itemize}
        \item \parencite{JEONG20112231} Instead of providing a universal bound or step condition the authors come up with a temporal weighting function and apply it to the cost matrix before calculating the accumulated cost matrix. For temporal weights the authors use a generalized logistic function that uses two parameters to center and scale the output of the function. The scale parameter is optimized on a validation set while the Performance is evaluated on a test set.
        \item \parencite{Maus_et_al_2016} This paper presents a time-weighted version of the dynamic time warping (DTW) method for land-use and land-cover classification using remote sensing image time series. Authors of the dtwSat package. Uses logistic-based time weights to temporally align signals from satelite imagery before classification decision. Supervised learning with manually labeled examples. The approach in this paper is copied by \parencite{Chaves_et_all_2021_Brazil_Crop_id}, \parencite{XiaoXingyuan2023LCCU}, \parencite{NarinO.G.2022URAN}, \parencite{QuXuzhou2023Mmls}
        \item \parencite{Ito_Sakemoto_2020} Propose a Multinomial DTW (MDTW) to directly estimate any existing lead/lag relationship between pairs of financial assets trading at high-frequency. They compare their approach to existing methods in the literature. Evidence from this paper (See table \ref{tbl:correlation_to_log_dtw_regression}) supports our analysis that the variance of DTW increases as the correlation between asset movements decreases. This paper uses simulates stock pairs evolutions with a bivariate brownian motion governed by $\rho$ and $\sigma^{2}$. DTW is calculated on the (stationary) log return series, something that differentiates this work from mine. 
    \end{itemize}
    \item Weighted Distance
    \begin{itemize}
        \item \parencite{JEONG20112231}
    \end{itemize}
    \item Counter-argument to using DTW on financial data - Hypothesis: Unit roots cause issues even controlling for different time series scales.
    \begin{itemize}
        \item \parencite{9145837_Entropic_DTW_Fin_Networks} graph approach to the variance-covariance 
    \end{itemize}
\end{enumerate}

\parencite{ElsingAgon2012} view the analysis of time series data from a data mining perspective. The authors provide a detailed survey of time series representations, various distance metrics for measuring association, and proper ways to index time series data for efficient querying and information retrieval. The distance measure are classified into four categories: shape, edit, feature, and structure-based.


\parencite{Mueller2007} provides an excellent summary of the principles of DTW and discuss several extensions with respect to the local and global parameters of the technique.





\parencite{WangXieHanSun2012} apply DTW to measure the pair-wise similarity between 35 foreign currencies. The authors then utilize a minimum spanning trees (MST) -- a novel graph algorithm -- to document the structural changes in the dependencies of the FX market over three separate years.

\parencite{KotsifakosAthitsosPapapetrou_2011} are also motivitated from a data mining perspective and the ability to return relevant results from query to a large number of time series. The authors compare an example-base query using DTW with a model-based approach using Hidden Markov Models (HMM).

\parencite{KeoghRatanamahatana_2005} Provide a theory for the most efficient lower bound on the dtw value. The authors adapt Piecewise Aggregate Approximation (PAA) for comparing series in a time-warping context.

\parencite{DuyTakeuchi2023statistical} Rare paper dealing directly with statistical inference of the DTW measure.

\parencite{WangXieHanSun2012} TBD

Finance:

\parencite{HowardTalisAlexeev_2020} Unpublished manuscript attempt that applies a DTW step to a classic CAPM stock return analysis. The CAPM model states that an asset's return is governed by the following relationship:

\begin{equation}
    E[r_{j}] = r_{f} + \beta_{j} (E[r_{m}] - r_{f})
\end{equation}

Instead of running the above regression using  synchronous returns the authors first "align" the sequences using the DTW algorithm and then calculate the betas $\beta_{j} = Cov(r_{t_{l}}, r_{s_{l}}) / Var(r_{m})$. The authors note that after approaching the estimation of $\beta$ in this way

\section{Data}

Studying the companies that make up the Standard and Poor's 500 will provide a rich sample to look for comparisons. Three different time periods will be considered. The financial crisis of 2007-2008, the moderate economic period between 2015 and 2016, and the dramatic shut-down economy of 2019-2020. With a focus on the these three time periods, the market upheaval surrounding the financial crisis and Covid-19 serve as a natural experiment with the moderate-but-steady growth of 2015-2016 acting as a control. This is especially true of the Covid-19 timeframe. Some industries bounced back strongly (Technology) while others struggled to recover (Travel, Hospitality). These alternate reactions will ensure that there is enough heterogeneity in the "shape" of the stock values of these companies to make meaningful comparisons. Natural industry classifications will also provide a good reference point to any latent clustering that is discovered.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{SandP_missing_map_pptx.png}
    \caption{Missing map of the S\&P data set. The red horizontal bars demark the timeframes under consideration. Of the 505 tickers in the dataset, there are 423 with a full series stock values from 2007 onward, 480 from 2015 onward, and 494 from 2019 onward.}
    \label{fig:SandP_missing_map}
\end{figure}


The companies comprising the S\&P's Wikipedia page\footnote{https://en.wikipedia.org/wiki/List\_of\_S\&P\_500\_companies} as of February 2021 are included in this essay's analysis. Using an API made freely available by Alpha Advantage\footnote{https://www.alphavantage.co/}, daily historical series of the current companies in the index have been obtained. Dates range from Nov 1999 to Feb 2021. See Figure \ref{fig:SandP_missing_map} for a visual summary of the price history for the individual companies in the index.

\section{Distance Measures}

\subsection{ARMA-GARCH Benchmark} \label{sec:ARMAGARCH-benchmark}

To benchmark the competing measures, a procedure is designed to estimate a valid ARMA-GARCH model for the log returns of each member of the S\&P 500. After controlling for the conditional mean (ARMA) and the conditional variance (GARCH) of the stock's DGP, pair-wise correlation values are calculated on the standardized residuals for each stock. For model validation, I will follow the same process as in section 4.3 of \cite{DowiakTV-COP}. Each model is checked to see if it is well specified and that the model residuals are abiding by the independent and identical distribution assumptions.

Define the return as $X_{t} = \log \left(S_{t}\right) - \log\left(S_{t-1}\right)$, where $S_{t}$ is the spot price at time $t$. The conditional mean for the log-returns can be formulated by the following ARMA process:

\begin{equation} \label{eqn:marginalModel}
    x_{t} = \mu_{t} + \epsilon_{t}
\end{equation}

\begin{equation} \label{eq:conditional_mean}
    \mu_{t} = \mu(\phi, \theta, x_{\{s:\, s < t\}}, \varepsilon_{\{s:\, s < t\}}) = \phi_{0} + \sum_{i=1}^{p} \phi_{i} x_{t-i} + \sum_{j=1}^{q} \theta_{j} \, \varepsilon_{t - j} + \varepsilon_{t}
\end{equation}

where $\varepsilon_{s}$ is an innovation term that satisfies $E[\varepsilon_{s}] = 0$ and $E[\varepsilon^{2}_{s}] = \sigma^{2}_{s}$. The conditional volatility process for the models under consideration can be generalized with the following formula. Functions \emph{A}, \emph{B}, \emph{C} are generic stand-ins that will differ across the various GARCH specifications.

\begin{equation} \label{eq:conditional_var}
    \sigma^{2}_{t} = \emph{C}(\varepsilon^{2}_{t-1}, \, \sigma^{2}_{t-1}) + \sum_{j=1}^{m} \alpha_{j} \emph{A}(\varepsilon^{2}_{t - j}) + \sum_{i=1}^{k} \beta_{i} \emph{B}(\sigma^{2}_{t - i})
\end{equation}

The conditional mean and variance are used to center and scale the innovation terms.

\begin{equation}
    z_{t}(\phi, \theta, \alpha, \beta) = \frac{x_{t} - \mu(x_{t-1}, \phi, \theta)}{\sigma(x_{t-1}, \alpha, \beta)}
\end{equation}

By collecting all the parameters in the conditional mean and variance equations into one vector $\Delta = [\phi, \theta, \alpha, \beta]$, the functional form for the error terms $f$ can be written as a product of $\Delta$, the chosen error distribution $g$, and any necessary shape parameters $\lambda$ of the distribution:

\begin{equation} \label{eq:conditional_distr}
    f(x_{t} | \mu_{t}, \sigma^{2}_{t}, \lambda, \Delta) = \frac{1}{\sigma_{t}} g(z_{t} | \lambda, \Delta)
\end{equation}

The challenge for automating this process is two-fold. First, the size of the full model space is quite large. To list out all the dimensions that must be considered:

\begin{itemize}
    \item ARIMA model
        \begin{itemize}
            \item Constant term
            \item Autoregressive order (p)
            \item Moving average order (q)
            \item Order of integration (d)
            \item Seasonal autoregressive order (P)
            \item Seasonal moving average order (Q)
            \item Order of seasonal integration (D)
        \end{itemize}
        \item GARCH model
        \begin{itemize}
            \item Constant term
            \item Autoregressive order (m)
            \item Moving average order (k)
            \item GARCH specification
            \item Error distribution
        \end{itemize}

\end{itemize}

A full grid-search over these dimensions is time-consuming and impractical. To reduce the number of specifications in the model set, the fitting procedure takes the following divide-and-conquer approach.

\begin{enumerate}
    \item \textbf{Estimate the conditional mean independently of the variance model.} Leveraging the work done by Hyndman and Khandakar \cite{HyndmanKhandakar2008AutoArima}, a step-wise strategy is used to search through the ARIMA model space for the best fit, which is evaluated via the Akaike information criterion (AIC). This is accomplished by using the Forecast package \cite{RForecast} running in the R statistical language \cite{RBase}. The following decisions are made: 
    
    \begin{itemize}
        \item Always include a constant term
        \item Set the integration terms to zero: $d = D = 0$
    \end{itemize}
    
    \item \textbf{Estimate a set of GARCH models.} Set the conditional mean model to the specification found in step 1. Then iterate over every GARCH specification in the model set, re-estimating the combined ARMA and GARCH parameters at the same time for each conditional variance model. This is accomplished by using the rugarch package \cite{Rugarch}. The model set that is searched through considers the following dimensions:
    
    \begin{itemize}
        \item Always include a constant term
        \item ARCH specification: m = \{1, 2\}
        \item GARCH specification: k = \{1, 2\}
        \item Distributions: \emph{g} = \{Normal, Student-t, Skewed Student-t \cite{FernandezSteel1998}\}
        \item Model Specification: \emph{A}, \emph{B}, \emph{C} = \{Standard GARCH \cite{Bollerslev1986Garch}, gjr-GARCH \cite{GJR1993Garch}, Component GARCH \cite{EngleLee1993APA} \}
    \end{itemize}
    
    With these dimensions, a total of 36 volatility models are available to choose from.

    \item \textbf{Select the best model specification.} The fitted residuals of a model are checked against a battery of tests to confirm the independent and identical assumptions as well as to verify the correct distribution has been selected. The tests used include: (a) Moment LM tests to check for any remaining auto-correlation in the first four moments, (b) Kolmogorov-Smirnov test to check the residuals against the chosen theoretical distribution, (c) Hong and Li \cite{HongLi2005} non-parametric density test jointly for i.i.d and correct distribution specification, (d) Shapiro-Wilks \cite{ShapiroWilks1965} test for normality, and (e) Jarque-Bera \cite{JarqueBera1980} test for joint normality for skew and kurtosis.
    
    With these tests in hand, finding the best model reduces to selecting the GARCH specification that:
    
    \begin{itemize}
        \item Passes all five distributional tests
        \item Minimizes the Bayesian information criterion (BIC)
    \end{itemize}

    If no specifications pass all five tests, then the one that minimizes the BIC across the 36 candidate models is selected.

\end{enumerate}

The specification for the ARMA process is found first independently of the GARCH process. Once the AR(p) and MA(q) orders have been found, this specification is set and remains the same as different volatility models are estimated. Note that the parameter estimates are not held constant, just the specification. For each new GARCH fit, the ARMA parameters are all re-estimated.


\subsection{Dynamic Time Warping} \label{sec:DTW}

Dynamic time warping (DTW) is an alternative method for comparing the association between two discrete time series. It differs from Pearson's correlation measure in that the time indices between the two series at moments of comparison are not constrained to equal each other -- like in Equation (\ref{eq:pearsons_rho}). Time is allowed to stretch and compress before a local cost\footnote{This is the languaged used in \cite{Mueller2007} but other authors use "association", "dissimilarity", or "distance" functions.} $c$ function is applied to the pairs of values from the two time series. This article will adhere to the classic definition of dynamic time warping. For notation this article this article borrows heavily from \cite{Mueller2007}.

\subsubsection{Algorithm} \label{sec:DTW_Algorithm}

Suppose there are two time series: $x_{t}$ for $t \in [1:T]$ and $y_{s}$ for $s \in [1:S]$. A \emph{warping path} is a sequence $\boldsymbol{p} = [p_{1},..., p_{L}]$ where each element is a mapping from the time index of one series to the other: $p_{l} = (t_{l}, \, s_{l}) \in [1:T] \times [1:S]$ for $l \in [1:L]$. For each point in the warping path $p_{l}$ there is a cost function quantifying the distance between the values of the series.

\begin{equation}
    c: x_{t_{l}} \times y_{s_{l}} \rightarrow \mathbb{R}_{\ge 0}
\end{equation}

The behaviour of the warping paths do follow some conditions, which are listed below:

\begin{itemize}
    \item \emph{Boundary Condition}: $p_{1} = (1, 1)$ and $p_{L} = (T, S)$ 
    \item \emph{Step-Size Condition}: $p_{l} - p_{l - 1} \in \{ (1, 0), (0, 1), (1, 1) \} $
\end{itemize}

The boundary condition requires that the first and last indices of the two series are mapped to each other. The step-size condition governs the evolution of the warping path. It ensures a non-decreasing monotonicity in the indices of \emph{both} series such that $t_{i} \le t_{j}$ and $s_{i} \le s_{j}$ if $i \le j$. A $T \times S$ cost matrix can be created that stores the associated cost between all values in the two series:

\begin{equation}
    \mathbf{C}(\boldsymbol{X}, \boldsymbol{Y}) = \left[ 
        \begin{array}{cccc}
            c(x_{T}, y_{1}) & c(x_{T}, y_{2}) & \cdots & c(x_{T}, y_{S}) \\ 
            \vdots          & \vdots          & \vdots & \vdots          \\
            c(x_{2}, y_{1}) & c(x_{2}, y_{2}) & \cdots & c(x_{2}, y_{S}) \\ 
            c(x_{1}, y_{1}) & c(x_{1}, y_{2}) & \cdots & c(x_{1}, y_{S})
    \end{array}\right]
\end{equation}

A warping path's total cost is the sum of the local costs it incurs as it travels from the start of the series (bottom left of $\boldsymbol{C}$) to their end (top right of $\boldsymbol{C}$):

\begin{equation} \label{eq:dtw_cost_matrix_definition}
    \mathbb{c}_{\boldsymbol{p}}(\boldsymbol{X}, \boldsymbol{Y}) = \sum^{L}_{l=1} c(x_{t_{l}},\, y_{s_{l}})
\end{equation}

There are many permissable\footnote{Permissable as governed by the boundary and step conditions} warping paths between two series. In fact the number of permissable paths has exponential growth in the values of $T$ and $S$. The aim during optimization is to find the warping path that minimizes the total cost. If the set of all warping paths are denoted $\mathbb{P}(\boldsymbol{X}, \boldsymbol{Y})$ then the value of the optimal warping path has the property

\begin{equation}
    \mathbb{c}_{\boldsymbol{p}^{*}}(\boldsymbol{X}, \boldsymbol{Y}) \le \mathbb{c}_{\boldsymbol{p}}(\boldsymbol{X}, \boldsymbol{Y}) \,\, \textrm{for all} \,\, \boldsymbol{p} \in \mathbb{P}(\boldsymbol{X}, \boldsymbol{Y})
\end{equation}

and the value of the DTW measure between $\boldsymbol{X}$ and $\boldsymbol{Y}$ is set to $\mathbb{c}_{\boldsymbol{p^{*}}}(\boldsymbol{X}, \boldsymbol{Y})$. An interested party could solve this optimization problem by estimating the total cost of all warping paths and select the one that minimizes this value. The challenge though is efficient computation. Since the number of warping paths grows exponentially the compuational time needed to check every warping path becomes problematic for large series. The proposed solution is to leverage \emph{dynamic programming} to reduce the computational time needed to find the optimal solution. Instead of dealing with exponential growth of permissable warping paths the DTW algorithm can find the optimal warping path in $\mathcal{O}(TS)$ calculations. To do so an \emph{accumulated cost matrix} $\boldsymbol{A}$ needs to be defined. The accumulated cost matrix has the same dimension as the cost matrix. Defining $A_{t,s}$ as the value of $\boldsymbol{A}$ at the $t^{th}$ row and the $s^{th}$ column of the accumulated cost matrix, the matrix has the following three identities:

\begin{equation}
    A_{t,1} = \sum^{t}_{k=1} c(x_{k}, y_{1}) \,\, \textrm{for} \,\, t \in [1:T]
\end{equation}

\begin{equation}
    A_{1,s} = \sum^{s}_{k=1} c(x_{1}, y_{k}) \,\, \textrm{for} \,\, s \in [1:S]
\end{equation}

\begin{equation}
    A_{t, s} = c(x_{t}, y_{s}) + \min\left(A_{t-1, s-1}, \, A_{t-1, s}, \, A_{t, s-1}\right)
\end{equation}

With this definition of the accumulative cost matrix the optimal warping path can be found by the following recursive procedure:

\begin{enumerate}
    \item Set $p_{L} = (T, S)$
    \item Given $p_{l} = (t, s)$ select $p_{l - 1} = \begin{cases} (1, s-1) & \textrm{if } t=1 \\ (t-1,1) & \textrm{if } s=1 \\ 
                                            \argmin \left[ A_{t-1, s-1},\, A_{t, s-1},\, A_{t-1, s} \right] & \textrm{otherwise} \end{cases}$
\end{enumerate}

\subsubsection{Common Extensions}

There are numerous modifications of the original DTW diverse set of industries and domains but applications to financial or economics are relatively sparse. Most are applied to:


The definition of the DTW in section \ref{sec:DTW_Algorithm} is an unconstrained one without restrictions beyond the boundary and step-size conditions. Most applications of dynamic time-warping constrain the extent of time distoration in some way. From a finance and macroeconomic perspective there are reasonable arguments for a limit as well. New information in markets dissipates over time.

Unit root behavior is a common challenge in the finance and macro-economic fields and it's presence brings unique challenges to interpreting the DTW value and making reliable inferences about stock behaviour. 

\begin{itemize}
    \item Temporal weight approaches
        \begin{itemize}
            \item \parencite{JEONG20112231}
            \item This article if we choose a weighting function suitable for 
        \end{itemize}
    \item Step-Size Constraints
        \begin{itemize}
            \item Sakoe and Chiba (1978)
        \end{itemize}
    \item Global Bounds
        \begin{itemize}
            \item Sakoe-Chiba Band, Itakura Parallelogram
        \end{itemize}
\end{itemize}

This article uses the Sakoe Chiba Band with a 15 day window (3 weeks of daily trading data)

\subsubsection{Example}

To put these concepts into practice the dynamic time warping algorithm will be demonstrated on the stock prices for Agilent Technologies (A) and General Electric (GE) from the first half of 2019. Instead of the original price level the series will be standardized by setting the price on January 2, 2019 equal to one. Figure \ref{fig:dtw_plot_stock_price_index_matching} summarizes the price movement of these two stocks and shows which price values are matched together by their optimal warping path. Figure \ref{fig:dtw_plot_warping_between_agilent_and_ge} displays the full optimal warping paths and discusses the influence of boundary and step-size conditions on it's shape. Figure \ref{fig:dtw_plot_cost_and_accum_cost} displays the cost and accumulated cost matrices for the first ten days of the two series. The optimal warping path is annotated in both.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dtw_plot_stock_price_index_matching.jpeg}
    \caption{This figures shows a normalized view of the Agilent and GE stock price for the first half of 2019. The normalization divides the stock price by it's value on the first market day of the year, January 2nd, 2019. Dynamic time warping is applied on the standardized series. The total length is 124 days ending on June 28th. The dashed gray lines connect elements of the index pair in the optimized warping path. The DTW finds the warping path that has the least total cost.}
    \label{fig:dtw_plot_stock_price_index_matching}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dtw_plot_warping_between_agilent_and_ge.jpeg}
    \caption{This figure displays the full warping path between Agilent and GE stock prices in the first half of 2019. The two peripheral images show their respective standardized stock prices while the central image traces the warping path as it connects the indicies of the two series. The influence of the step-condition is seen immediately in the marginal increments of the path. They are non-decreasing. This allows time to "stretch" but not be put into reverse. At the end of the warping path (top right part of the image) the boundary condition becomes binding--seen by the vertical line at the tail end--as the last index in GE's series is reached before the last index of Agilent's.}
    \label{fig:dtw_plot_warping_between_agilent_and_ge}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dtw_plot_cost_and_accum_cost.jpeg}
    \caption{The left image is the cost matrix for the first ten values of the Agilent and GE stock prices after standardization. The local cost function is the euclidean distance. The right image is the accumulative cost matrix for the same sequence of prices. The optimal warping path is annotated with white outlines. The gray shading tracks the range of values in each matrix. Darker shades of gray represent smaller values while lighter shades of gray represents larger values. Note that the scale of shading is not uniform across images.}
    \label{fig:dtw_plot_cost_and_accum_cost}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{dtw_plot_cost_matrix_topography.jpeg}
%     \caption{This figure displays the optimal alignment between Aligent and GE stock return series. The topology lines are based on the % cost matrix. From this view the warping path (in blue) is a "valley of least cost" traversing the local cost landscape.}
%     \label{fig:dtw_plot_cost_matrix_topography}
% \end{figure}


\pagebreak


\section{Pair Trading} \label{sec:PairTrading}


One way to compare these measures is putting them to practical use. In this section a simple trading strategy is described that centers around finding "similar" pairs of stocks and trading off the expectation that any major short term deviations in their (normalized) prices are eventually unwound before the end of the trading period. For this trade strategy the critical decision is how to select pairs of stocks and what criterion to use for similarity. In this section we directly compare the returns of portfolios formed using correlation against portfolio returns that use dynamic time warping and euclidean distance to measure similarity. If the returns to using dynamic time warping to form trading pairs are materially different than returns from a correlation based approach it could help us understand whether this metric can provide meaningful information to use in the analysis of stock returns.

The specifics of this pair trading strategy are the same as the approach taken by \cite{Gatev_et_al_2006}. The execution of this strategy takes place in two parts: a \textit{formation period} where a trader will create a portfolio of stock pairs and the \textit{trading period} where the trader will buy and sell those pairs according to a defined set of market signals. This experiment is not carried out because of high expected returns, especially in the context of our modern trading environment. The rise of high frequency trading in the 1990's and 2000's has created the financial infrastructure that sees arbitrage opportunities resolved in microseconds\footcite[See][for a more detailed ]{Aquilina_et_al_2021}. This article studies stock returns captured at a daily frequency and is at a natural disadvantage in comparison. In addition \cite{Gatev_et_al_2006} was published close to two decades ago in a widely read finance journal making its recipe part of the established canon. The benefits to using this particular trading strategy is that it provides a suitable context to directly compare the impact of using dynamic time warping -- instead of correlation -- to identify the "similar" pairs to trade.

\subsection{Trading Strategy}

The following subsections provide more detail about the formation and trading periods and how the different measures of association will be compared.

\subsubsection{Formation Period}

During the formation period the trader compares historical stock returns and uses a measure of similarity to form a portfolio of N pairs of the closest related stocks. In this article a one year time span serves as the duration of the formation period. Before stock comparisons are made prices are first transformed from their nominal price to a measure of their cumulative return. For any date $t$ a stock's standard price is found by using the following calculation:

\begin{equation} \label{eq:standard_price}
    p_{t} = \prod_{1}^{t} (1 + r_{t})
\end{equation}

After pair selection the trader again uses the price history in the formation period to calculate a critical value for the price differential experienced by each pair. This article follows \cite{Gatev_et_al_2006} and uses a two standard deviation threshold as a signal to open a position on the pair during the trading period. That is if $p_{t}$ and $q_{t}$ are the two standardize prices for a pair and $\boldsymbol{d}$ is the vector of absolute price differentials with elements $d_{t} = |p_{t} - q_{t}|$, then over the entire formation period the threshold that acts as a signal to open a trading position is set to $\boldsymbol{\bar{d}} + 2 \sigma_{\boldsymbol{d}}$.

After price normalization, four different portfolio collections are created based on how we want to measure similarity. In this article, the cost function used in the dynanimc time warping function is the $L^{2}$-norm. (see section \ref{sec:DTW} and equation \ref{eq:dtw_cost_matrix_definition}) is 


As mentioned before the central task of this section is to compare the performance of portfolios whose pairs are selected using correlation versus pairs that are formed using dynamic time warping. gainst  this section is evaluating is how to form There are different methods for chosing the association measure:

\begin{enumerate}
    \item Unadjusted Correlation: Measure closeness via correlation. Use static (unconditional) distributions and a two standard deviation threshold on the cumulative return differential as a signal to open positions.
    \item Model Correlation 1: Model the returns using the ARIMA approach. This approach will enable the trader to study the return distribution of the market and possibly control for  non-normality . Measure closeness on the fitted residuals. Use rolling mean/variance values to calculate adjusted threshold during trading period
    \item Model Correlation 2: Model the univariate returns using the ARIMA-GARCH framework. This approach will enable the trader to study the return distribution of the market and control for any deviations from normality that commonly occur in finance. These deviations (from the Guassian benchmark) include return distributions that exhibit fat tails, asymmetry, and/or volatility clustering. For a given formation period we only consider stocks that have successfully passed a battery of diagnostics that test the fit of the ARIMA-GARCH model. In addition, this restriction to stocks with well-fitted ARIMA-GARGH specifications applies to portfolios formed using any one of the four approaches.
    \item Euclidean Distance: Used as a practical baseline for multiple reasons. First, \cite{Gatev_et_al_2006} using euclidean distance in their pair trading strategy and including it will allow for direct comparison. Second, euclidean distance can serve as benchmark to test what gains can be acheived if any time elasticity is present in the a stock pair's return evolution.
    \item Dynamic Time Warping: Calculate the DTW metric between the cumulative returns of the pairs. Use synchronous L1 value and the two standard deviation signal to open positions.
\end{enumerate}

\subsubsection{Trading Period}

Given a single stock pair in a trader's portfolio a position is opened if the price deviation between the two stocks exceeds the two standard deviation threshold. Once triggered the trader goes long in the lower priced stock and takes a short position in the higher priced stock. The positions are unwound when the prices come back to parity, profits are recorded, and the trader stands ready to open another position given a fresh buy-signal. That is a single pair can be opened and closed multiple times during a single trading period. If this happens this leads to multiple revenue streams over the duration of the trading period. The total return of this pair is calculated as the compounded return of these multiple revenue streams. Take note that the inclusion of a stock pair in the trader's portfolio does not mean that it must be traded. If the differential in the standard prices of the pair never deviate beyond the two standard deviation limit then a position in a pair will never be opened.

\subsection{Post Trade Analysis}


Hot takes on the return table:
\begin{itemize}
    \item For the correlation-based pair strategy there is benefit diversification. The standard deviation of the portfolio return decreases going from the top 25 to top 50. This does not hold true for the return distribution of the portfolio created with pairs formed with dtw.
    \item As expected there does not appear to be significant arbitrage opportunities arbitrage opportunities 
\end{itemize}
- For the correlation-based pair strategy there is benefit to 


\begin{table}[hp]
    \fontsize{8pt}{8pt}\selectfont
    \centering
    \begin{tabular}{l r r l r r r r r r}
        \multicolumn{9}{l}{\textbf{I. Returns on Committed Equity}} \\
        \# Pairs & Pair Strategy & Mean & Std Err{$^{1}$} & Std Dev & Median & Skew & Kurtosis & Min & Max \\
        \hline
        \vspace{-1mm} \\
        Top 25    & Unadjusted cor &  0.0018 & 0.0063          & 0.030 &  0.0011 & -0.873 & 5.039 & -0.085 & 0.061 \\
        Top 25    & Model cor      & -0.0033 & 0.0057          & 0.027 &  0.0003 & -1.199 & 4.417 & -0.078 & 0.032 \\
        Top 25    & DTW            &  0.0185 & 0.0085{$^{*}$}  & 0.041 &  0.0096 &  0.695 & 2.992 & -0.041 & 0.114 \\
        Top 25    & Euclidean      &  0.0064 & 0.0071          & 0.034 & -0.0074 &  0.945 & 3.359 & -0.040 & 0.092 \\
        \vspace{-1mm} \\
        Top 50    & Unadjusted cor & -0.0003 & 0.0049          & 0.023 & -0.0001 &  0.028 & 1.918 & -0.038 & 0.043 \\
        Top 50    & Model cor      & -0.0011 & 0.0052          & 0.025 & -0.0054 &  0.226 & 1.935 & -0.036 & 0.044 \\
        Top 50    & DTW            &  0.0084 & 0.0070          & 0.034 &  0.0032 &  0.512 & 2.999 & -0.057 & 0.084 \\
        Top 50    & Euclidean      &  0.0064 & 0.0072          & 0.035 & -0.0009 &  1.136 & 3.505 & -0.042 & 0.091 \\
        \vspace{-1mm} \\
        Top 100   & Unadjusted cor &  0.0013 & 0.0058          & 0.028 &  0.0004 &  0.430 & 2.357 & -0.037 & 0.067 \\
        Top 100   & Model cor      &  0.0007 & 0.0061          & 0.029 & -0.0017 &  0.412 & 2.735 & -0.051 & 0.071 \\
        Top 100   & DTW            &  0.0083 & 0.0082          & 0.039 &  0.0034 &  0.897 & 4.070 & -0.062 & 0.109 \\
        Top 100   & Euclidean      &  0.0073 & 0.0078          & 0.037 &  0.0057 &  0.763 & 3.619 & -0.045 & 0.107 \\
        \vspace{-1mm} \\
        \hline
        \vspace{1 mm} \\
        \multicolumn{9}{l}{\textbf{II. Returns on Fully Invested Equity}} \\
        \# Pairs  & Pair Strategy & Mean & Std Err{$^{1}$} & Std Dev & Median & Skew & Kurtosis & Min & Max \\
        \hline
        \vspace{-1mm} \\
        Top 25    & Unadjusted cor &  0.0100 & 0.0106          & 0.051 &  0.0021 &  0.057 & 4.071 & -0.112 & 0.132 \\
        Top 25    & Model cor      &  0.0034 & 0.0104          & 0.050 &  0.0005 &  0.434 & 3.994 & -0.093 & 0.133 \\
        Top 25    & DTW            &  0.0189 & 0.0086{$^{*}$}  & 0.042 &  0.0096 &  0.721 & 3.062 & -0.041 & 0.119 \\
        Top 25    & Euclidean      &  0.0067 & 0.0072          & 0.035 & -0.0075 &  0.919 & 3.245 & -0.040 & 0.092 \\
        \vspace{-1mm} \\
        Top 50    & Unadjusted cor &  0.0015 & 0.0078          & 0.038 & -0.0006 &  0.041 & 1.701 & -0.060 & 0.064 \\
        Top 50    & Model cor      &  0.0010 & 0.0089          & 0.043 & -0.0086 &  0.343 & 2.234 & -0.065 & 0.097 \\
        Top 50    & DTW            &  0.0084 & 0.0071          & 0.034 &  0.0033 &  0.457 & 2.973 & -0.060 & 0.084 \\
        Top 50    & Euclidean      &  0.0064 & 0.0073          & 0.035 & -0.0009 &  1.116 & 3.435 & -0.042 & 0.091 \\
        \vspace{-1mm} \\
        Top 100   & Unadjusted cor &  0.0032 & 0.0095          & 0.046 &  0.0007 &  0.134 & 1.556 & -0.062 & 0.073 \\
        Top 100   & Model cor      &  0.0020 & 0.0096          & 0.046 & -0.0022 &  0.127 & 1.758 & -0.072 & 0.077 \\
        Top 100   & DTW            &  0.0082 & 0.0082          & 0.040 &  0.0035 &  0.845 & 4.017 & -0.064 & 0.109 \\
        Top 100   & Euclidean      &  0.0073 & 0.0078          & 0.037 &  0.0057 &  0.740 & 3.556 & -0.046 & 0.107 \\
        \vspace{-1mm} \\
        \hline
        \vspace{1 mm} \\
        \multicolumn{9}{l}{\textbf{III. Baseline Long Position}} \\
        \# Pairs & Pair Strategy & Mean & Std Err{$^{1}$} & Std Dev & Median & Skew & Kurtosis & Min & Max \\
        \hline
        \vspace{-1mm} \\
        Top 25    & Unadjusted cor & 0.0557 & 0.0483          & 0.232 & 0.1049 & -0.234 & 2.753 & -0.463 & 0.513 \\
        Top 25    & Model cor      & 0.0487 & 0.0490          & 0.235 & 0.0815 & -0.003 & 3.256 & -0.472 & 0.597 \\
        Top 25    & DTW            & 0.0899 & 0.0310{$^{**}$} & 0.149 & 0.1371 & -1.196 & 4.021 & -0.333 & 0.266 \\
        Top 25    & Euclidean      & 0.0970 & 0.0315{$^{**}$} & 0.151 & 0.1418 & -0.892 & 3.847 & -0.314 & 0.364 \\
        \vspace{-1mm} \\
        Top 50    & Unadjusted cor & 0.0648 & 0.0472          & 0.227 & 0.1161 & -0.633 & 2.837 & -0.490 & 0.413 \\
        Top 50    & Model cor      & 0.0596 & 0.0472          & 0.227 & 0.0947 & -0.466 & 2.674 & -0.470 & 0.431 \\
        Top 50    & DTW            & 0.0711 & 0.0311{$^{**}$} & 0.149 & 0.1266 & -1.079 & 3.986 & -0.339 & 0.274 \\
        Top 50    & Euclidean      & 0.0895 & 0.0302{$^{**}$} & 0.145 & 0.1238 & -0.947 & 3.559 & -0.302 & 0.304 \\
        \vspace{-1mm} \\
        Top 100   & Unadjusted cor & 0.0700 & 0.0450          & 0.216 & 0.1292 & -0.809 & 3.100 & -0.488 & 0.358 \\
        Top 100   & Model cor      & 0.0715 & 0.0458          & 0.220 & 0.1253 & -0.648 & 2.952 & -0.472 & 0.403 \\
        Top 100   & DTW            & 0.0875 & 0.0312{$^{**}$} & 0.150 & 0.1335 & -1.027 & 3.882 & -0.334 & 0.288 \\
        Top 100   & Euclidean      & 0.0912 & 0.0307{$^{**}$} & 0.148 & 0.1300 & -0.943 & 3.600 & -0.312 & 0.287 \\
        \vspace{-1mm} \\
        \hline
    \end{tabular}
    \caption{Average Annual Return Distribution}
    \begin{tablenotes}
        \item{\footnotesize Three different measures of portfolio returns are summarized in this table. Section I records committed returns where the total return of the portfolio during the trading period is divided by the number of pairs selected to trade. Section II of the table records fully invested returns where the total return of the portfolio during the trading period is devided by the number of pairs selected to trade. Section III records the annual return of the portfolio if the trader simply goes long in each pair at the beginning of the trading period.}
        \item{\footnotesize \textbf{Signif. Codes:} 0 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1}
        \item{\footnotesize $^{1}$Standard errors are Newey-West estimates and significant values are } 
    \end{tablenotes}
\end{table}


\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{30}{1em}}}% no optional argument here, please!

\begin{table}[hp]
    \fontsize{10pt}{10pt}\selectfont
    \centering
    \begin{tabular}{l r r r r r r r r r r r | r r}
          & \rot{Communication Services} & \rot{Consumer Discretionary} & \rot{Consumer Staples} & \rot{Energy} & \rot{Financials} & \rot{Health Care} & \rot{Industrials} & \rot{Information Technology} & \rot{Materials} & \rot{Real Estate} & \rot{Utilities} & \underline{Count} & \underline{\%} \\
                                 &    &    &    &    &    &    &    &    &    &    &    &     &      \\
        Communication Services   & 17 &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &  34 &  3.1 \\
        Consumer Discretionary   &  . & 30 &  . &  . &  . &  . &  . &  . &  . &  . &  . &  60 &  5.5 \\
        Consumer Staples         &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &   . &    . \\
        Energy                   &  . &  . &  . & 47 &  . &  . &  . &  . &  . &  . &  . &  94 &  8.5 \\
        Financials               &  . &  . &  . &  . &147 &  . &  . &  . &  . &  . &  . & 294 & 26.7 \\
        Health Care              &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &   . &    . \\
        Industrials              &  . &  . &  . &  . &  . &  . & 10 &  . &  . &  . &  . &  20 &  1.8 \\
        Information Technology   &  . &  . &  . &  . &  . &  . &  . & 36 &  . &  . &  . &  72 &  6.5 \\
        Materials                &  . &  . &  . &  . &  . &  . &  . &  . &  5 &  . &  . &  12 &  1.1 \\
        Real Estate              &  . &  . &  . &  . &  . &  . &  . &  . &  2 &161 &  . & 324 & 29.5 \\
        Utilities                &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . & 95 & 190 & 17.3 \\
        \vspace{0.25 mm} \\
        \multicolumn{14}{c}{\textbf{Unadjusted Correlation}} \\
        \vspace{1 mm} \\
                                 &    &    &    &    &    &    &    &    &    &    &    &     &       \\
        Communication Services   & 17 &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &  34 &  3.1 \\
        Consumer Discretionary   &  . & 36 &  . &  . &  . &  . &  . &  . &  . &  . &  . &  72 &  6.5 \\
        Consumer Staples         &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &   . &    . \\
        Energy                   &  . &  . &  . & 52 &  . &  . &  . &  . &  . &  . &  . & 104 &  9.5 \\
        Financials               &  . &  . &  . &  . & 117&  . &  . &  . &  . &  . &  . & 234 & 21.3 \\
        Health Care              &  . &  . &  . &  . &  . &  1 &  . &  . &  . &  . &  . &   2 &  0.2 \\
        Industrials              &  . &  . &  . &  . &  . &  . & 13 &  . &  . &  . &  . &  26 &  2.4 \\
        Information Technology   &  . &  . &  . &  . &  . &  . &  . & 44 &  . &  . &  . &  88 &  8.0 \\
        Materials                &  . &  . &  . &  . &  . &  . &  . &  . & 10 &  . &  . &  23 &  2.1 \\
        Real Estate              &  . &  . &  . &  . &  . &  . &  . &  . &  3 &150 &  . & 303 & 27.5 \\
        Utilities                &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &107 & 214 & 19.5 \\
        \vspace{0.25 mm} \\
        \multicolumn{14}{c}{\textbf{Model Correlation}} \\
        \vspace{1 mm} \\
        \hline
    \end{tabular}
    \caption{Sector Distribution of Portfolio Pairs: Top 25 by Correlation}
    \begin{tablenotes}
        \item{\footnotesize This table tabulates the sector distribution of all pairs in the annual portfolios constructed from 2011 to 2022. Two views are presented. On the left is the cross sector count of \textit{pairs}. On the right is the (marginal) sector count for all individual \textit{stocks} in the set of portfolios. Intra-industry pairs are the rule using either correlation based pairing strategy. The only few exceptions that are pairs between Real Estate and Materials stocks. Taken together the Financial, Real Estate, and Utility sectors account for 73.5 and 68.3 percent out of all stocks chosen in the unadjusted and modeled portfolios, respectively.}
    \end{tablenotes}
    \label{tbl:pair_cross_sector_tabulation:correlation}
\end{table}


\begin{table}[!htb]
    \fontsize{10pt}{10pt}\selectfont
    \centering
    \begin{tabular}{l r r r r r r r r r r r | r r}
          & \rot{Communication Services} & \rot{Consumer Discretionary} & \rot{Consumer Staples} & \rot{Energy} & \rot{Financials} & \rot{Health Care} & \rot{Industrials} & \rot{Information Technology} & \rot{Materials} & \rot{Real Estate} & \rot{Utilities} & \underline{Count} & \underline{\%} \\ 
                                 &    &    &    &    &    &    &    &    &    &    &     \\
        Communication Services   & 14 &  . &  . &  . &  . &  . &  . &  . &  . &  . &   . &  44 &  4.0 \\
        Consumer Discretionary   &  . &  3 &  . &  . &  . &  . &  . &  . &  . &  . &   . &  24 &  2.2 \\
        Consumer Staples         &  2 &  3 & 13 &  . &  . &  . &  . &  . &  . &  . &   . &  92 &  8.4 \\
        Energy                   &  . &  . &  1 &  5 &  . &  . &  . &  . &  . &  . &   . &  21 &  1.9 \\
        Financials               &  2 &  2 & 13 &  . & 50 &  . &  . &  . &  . &  . &   . & 163 & 14.8 \\
        Health Care              &  3 &  . & 12 &  1 & 12 &  6 &  . &  . &  . &  . &   . &  59 &  5.4 \\
        Industrials              &  3 &  4 &  7 &  3 &  8 &  6 & 20 &  . &  . &  . &   . &  91 &  8.3 \\
        Information Technology   &  2 &  1 &  1 &  2 &  9 &  3 &  6 &  2 &  . &  . &   . &  31 &  2.8 \\
        Materials                &  . &  3 &  1 &  2 &  4 &  1 &  6 &  2 &  2 &  . &   . &  28 &  2.6 \\
        Real Estate              &  . &  1 &  1 &  . &  2 &  . &  2 &  . &  2 & 49 &   . & 115 & 10.5 \\
        Utilities                &  4 &  4 & 25 &  2 & 11 &  9 &  6 &  1 &  3 &  9 & 178 & 430 & 39.2 \\
        \vspace{0.25 mm} \\
        \multicolumn{14}{c}{\textbf{Dynamic Time Warping}} \\
        \vspace{1 mm} \\
                                 &    &    &    &    &    &    &    &    &    &    &    \\
        Communication Services   & 14 &  . &  . &  . &  . &  . &  . &  . &  . &  . &   . &  38 &  3.5 \\
        Consumer Discretionary   &  . &  . &  . &  . &  . &  . &  . &  . &  . &  . &   . &   8 &  0.7 \\
        Consumer Staples         &  1 &  3 & 19 &  . &  . &  . &  . &  . &  . &  . &   . &  86 &  7.8 \\
        Energy                   &  . &  . &  . &  7 &  . &  . &  . &  . &  . &  . &   . &  18 &  1.6 \\
        Financials               &  2 &  1 & 13 &  . & 71 &  . &  . &  . &  . &  . &   . & 191 & 17.4 \\
        Health Care              &  1 &  . &  8 &  1 &  7 &  4 &  . &  . &  . &  . &   . &  34 &  3.1 \\
        Industrials              &  1 &  1 &  2 &  . & 10 &  3 & 17 &  . &  . &  . &   . &  70 &  6.4 \\
        Information Technology   &  1 &  . &  1 &  2 &  3 &  1 &  3 &  2 &  . &  . &   . &  18 &  1.6 \\
        Materials                &  . &  1 &  . &  1 &  3 &  . &  9 &  2 &  3 &  . &   . &  26 &  2.4 \\
        Real Estate              &  . &  1 &  . &  . &  2 &  . &  2 &  . &  1 & 72 &   . & 156 & 14.2 \\
        Utilities                &  4 &  1 & 20 &  . &  8 &  5 &  5 &  1 &  3 &  6 & 200 & 453 & 41.3 \\
        \vspace{0.25 mm} \\
        \multicolumn{14}{c}{\textbf{Euclidean Distance}} \\
        \vspace{1 mm} \\
        \hline
    \end{tabular}
    \caption{Sector Distribution of Portfolio Pairs: Top 25 by $L^{2}$ Norm}
    \begin{tablenotes}
        \item{\footnotesize This table tabulates the sector distribution of all pairs in the annual portfolios constructed from 2011 to 2022. Two views are presented. On the left is the cross sector count of \textit{pairs}. On the right is the (marginal) sector count for all individual \textit{stocks} in the set of portfolios. Diversification away from intra-industry pairs is the main differentiator between the portfolios summarized here versus those using correlation as captured in table \ref{tbl:pair_cross_sector_tabulation:correlation}. Taken together the Financial, Real Estate, and Utility sectors account for 64.8 and 72.9 percent out of all stocks chosen in the time-warped and euclidean portfolios, respectively.}
    \end{tablenotes}
\end{table}

\begin{landscape}
    \begin{figure}[hp]
        \includegraphics[width=1.3\textwidth]{Annual_Returns_to_Committed_Capital.png}
        \caption{Barcharts indicating the annual committed returns of the trading strategy under different buy signals and portfolio sizes.}
        \label{fig:annual_returns_to_committed_capital_by_buy_signal}
    \end{figure}
\end{landscape}

\section{Simulation Results}
In order to gauge the relationship between the DGP process of two time series and their resulting DTW value we turn to simulation. For the first simulation a stationary process is developed with only two factors: an intercept and an error term.

\begin{equation}
    X_{t} = \omega + \epsilon_{t} \qquad \epsilon_{t} \thicksim \mathcal{N}(0, \sigma^{2})
\end{equation}

This stationary process represents a simplified ARIMA-GARCH model, replacing the conditional mean and variance with their unconditional equivalents. After fixing parameter values ($\omega_{A}$, $\sigma_{A}$, $\omega_{B}$, $\sigma_{B}$) we can sample from the normal distribution to produce stationary time series of arbitrary length. By sampling a series each from specifications A and B, then calculating the dynamic time-warping distance between the two, we can trace the effect of the parameters on the value of the DTW metric. By repeating this process N times we can build up a distribution of these values. A similar apprach can be used to capture the null distribution of the dynamic time-warping metric for two unit root processes that obey the following specification.

\begin{equation} \label{eq:random_walk}
    X_{t} = \omega + X_{t-1} + \epsilon_{t} \qquad \epsilon_{t} \thicksim \mathcal{N}(0, \sigma^{2})
\end{equation}

From equation (\ref{eq:random_walk}) at any time $t$ we know both the expectation and the variance of both random walk processes:

\begin{equation}
    X_{i,t} \thicksim \mathcal{N}(t\omega_{i}, \, t\sigma_{i}^{2})
\end{equation}

A random walk temporal weighting function could have the following form:

\begin{equation}
    \omega_{t, s} = \lvert t - s \rvert \sqrt{ \sigma_{A,\, t} \sigma_{B,\, s} }
\end{equation}

\begin{equation}
    \omega_{t, s} = \rho \lvert t - s \rvert \sigma_{A,\, t} \sigma_{B,\, s}
\end{equation}

\begin{equation}
    \omega(t, s) = \sum_{k=1}^{|t-s|} \beta^{|t-s|} \left( \sigma^{2}_{A,\, t} + \sigma^{2}_{B,\,s} \right)
\end{equation}


\subsection{A Relationship Between Correlation and DTW?}

In section \ref{sec:PairTrading} a total of 10,018 ARIMA-GARCH models were estimated during the formation periods from 2000 to 2021 where each model was trained on a single year's worth of trading data. From these ten thousand marginal models a total of 2,199,276 pairs were formed and estimates of the pair's correlation and dynamic time warping distance were calculated over the duration of the study.

These estimated models, combined with Copula theory, can be leveraged to study the range of dynamic time warping values that can be expected for a pair of stocks with a specific level of correlation (See section 2.3.1 of \cite{DowiakTV-COP} for more details on copulas and their applications). The simulation exercise has the following steps.

\begin{enumerate}
    \item For two arbitrary stocks, $A$ and $B$, the conditional mean, conditionl variance, and conditional density of the returns are recorded for each asset: $\hat{\mu}_{A,\,t}$, $\hat{\sigma}^{2}_{A,\,t}$, $\hat{g}_{A}$, and $\hat{\mu}_{B,\,t}$, $\hat{\sigma}^{2}_{B,\,t}$, $\hat{g}_{B}$ (see equations \ref{eq:conditional_mean}, \ref{eq:conditional_var}, and \ref{eq:conditional_distr} from section \ref{sec:ARMAGARCH-benchmark}).
    \item The probability integral transform is used to reformulate each marginal model's fitted residuals as a sample from the uniform distribution.
    \begin{equation} \nonumber
        \hat{U}_{A} = \hat{F}_{A}(s) = \int_{-\infty}^{s} \frac{1}{\hat{\sigma}_{A,\,t}} \hat{g}_{A} \left(z | \hat{\mu}_{t}\right) dz
    \end{equation}
    \begin{equation} \nonumber
        \hat{U}_{B} = \hat{F}_{B}(s) = \int_{-\infty}^{s} \frac{1}{\hat{\sigma}_{B,\,t}} \hat{g}_{B} \left(z | \hat{\mu}_{t}\right) dz
    \end{equation}
    The residuals, in their uniform representation [($\hat{u}_{A,\,1}$, $\hat{u}_{B,\,1}$), $\cdots$, ($\hat{u}_{A,\,T}$, $\hat{u}_{B,\,T}$)], are used to estimate a bivariate t-Copula which is defined by it's correlation $\rho_{A,B}$ and degrees-of-freedom $\nu_{A,B}$ parameters.
    \item The estimated t-Copula is used as a data generating process to produce new sets of bivariate uniform distributions [($u^{(i)}_{A,\,1}$, $u^{(i)}_{B,\,1}$), $\cdots$, ($u^{(i)}_{A,\,T}$, $u^{(i)}_{B,\,T}$)]. Each observation $t$ in a new sample is centered and scaled according to the conditional mean and variance from the estimated ARIMA-GARCH models.
    \begin{equation} \nonumber
        r^{(i)}_{A,\,t} = \hat{F}^{-1}_{A} \left(u^{(i)}_{A,\,t}\,\,;\,\, \mu=\hat{\mu}_{A,\,t},\, \sigma^{2}=\hat{\sigma}^{2}_{A,\,t} \right)
    \end{equation}
    \begin{equation} \nonumber
        r^{(i)}_{B,\,t} = \hat{F}^{-1}_{B} \left(u^{(i)}_{B,\,t}\,\,;\,\, \mu=\hat{\mu}_{B,\,t},\, \sigma^{2}=\hat{\sigma}^{2}_{B,\,t} \right)
    \end{equation}
    \item After a new set of return series are generated they are transformed from returns to price level using the standard price equation \ref{eq:standard_price}.
    \item Repeat steps one through four to build up a distribution of dynamic time warping values for a pair of stock prices with a specific correlation value
\end{enumerate}

By marrying the conditional return models with a copula-based resampling approach many of the important characteristics of stock returns can be included in the monte carlo samples. These include auto-correlation, asymmetric returns, fat-tails, and volatility clustering observed in individual stocks as well as high levels of correlation in price movements between stocks over time. The original association and tail-dependency observed in the stock pairs is preserved by the copula while the unique idiosyncracies (See section \ref{sec:ARMAGARCH-benchmark}) of each stock is retained by the marginal models. This leaves only the natural variation of the distribution $g$ to drive the variation in the dynamic time warping values calculated in step 4. Assuming multivariate t-distributions for this simulation exercise is well founded. In the model selection process described in section \ref{sec:ARMAGARCH-benchmark} The t-distribution is chosen for all 10,018 marginal models in this study. The symmetric t-distribution is chosen 9324 times while the remaining 694 models are modeled with a skewed t-distribution.


\begin{table}
    \centering
    \begin{tabular}{l r l r l}
         & Estimate & Std Err & Estimate & Std Err \\
        \hline
        Intercept       &   3.496 &  0.014{$^{**}$} &   3.552 & 0.043{$^{**}$}  \\
        Abs Mean Diff   & 305.712 &  7.101{$^{**}$} & 275.777 & 7.383{$^{**}$}  \\
        Mean Variance   &   0.892 &  0.246{$^{**}$} &   0.490 & 0.244{$^{*}$}   \\
        1 - model cor 1 &  39.989 &  1.062{$^{**}$} &  50.439 & 1.344{$^{**}$}  \\
        1 - model cor 2 & -17.317 &  0.880{$^{**}$} & -18.608 & 0.916{$^{**}$}  \\
        Intra Sector    &  -0.019 &  0.030{$^{  }$} &   0.071 & 0.030{$^{*}$}   \\
        2001            &      -- &              -- &   0.058 & 0.055           \\
        2002            &      -- &              -- &  -0.266 & 0.074{$^{**}$}  \\
        2003            &      -- &              -- &   0.046 & 0.075           \\
        2004            &      -- &              -- &  -0.283 & 0.073{$^{**}$}  \\
        2005            &      -- &              -- &  -0.234 & 0.077{$^{**}$}  \\
        2006            &      -- &              -- &  -0.120 & 0.069{$^{+}$}   \\
        2007            &      -- &              -- &  -0.048 & 0.077           \\
        2008            &      -- &              -- &   0.146 & 0.060{$^{*}$}   \\
        2009            &      -- &              -- &   0.612 & 0.062{$^{**}$}  \\
        2010            &      -- &              -- &   0.092 & 0.062           \\
        2011            &      -- &              -- &   0.042 & 0.056           \\
        2012            &      -- &              -- &  -0.220 & 0.068{$^{**}$}  \\
        2013            &      -- &              -- &  -0.032 & 0.067           \\
        2014            &      -- &              -- &  -0.336 & 0.065{$^{**}$}  \\
        2015            &      -- &              -- &  -0.177 & 0.061{$^{**}$}  \\
        2016            &      -- &              -- &  -0.020 & 0.052           \\
        2017            &      -- &              -- &  -0.436 & 0.047{$^{**}$}  \\
        2018            &      -- &              -- &  -0.207 & 0.058{$^{**}$}  \\
        2019            &      -- &              -- &  -0.158 & 0.049{$^{**}$}  \\
        2020            &      -- &              -- &   0.256 & 0.054{$^{**}$}  \\
        2021            &      -- &              -- &  -0.040 & 0.047           \\
        2022            &      -- &              -- &   0.165 & 0.053{$^{**}$}  \\
        \vspace{-1mm} \\
    \end{tabular}
    \caption{Relationship between correlation and dynamic time warping}
    \begin{tablenotes}
        \item{\footnotesize yada yada yada}
        \item{\footnotesize \textbf{Signif. Codes:} 0 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1}
        \item{\footnotesize $^{1}$Standard errors are Newey-West estimates and significant values are } 
    \end{tablenotes}
    \label{tbl:correlation_to_log_dtw_regression}
\end{table}


\pagebreak


\begin{table}[!htb]
    \begin{center}
      \begin{tabular}{| r r r r | r r r r r r r |}
        \hline
        $\omega_{A}$ & $\omega_{B}$ & $\sigma^{2}_{A}$ & $\sigma^{2}_{B}$ & Min & 1st Qu. & Median & Mean & 3rd Qu. & Max & IQR \\
        \hline
        0  & 0 & 1  & 1  & 0.72  & 0.82  & 0.85  & 0.85  & 0.89  & 0.97  & 0.06 \\
        \hline
        0  & 0 & 1  & 2  & 0.86  & 1.04  & 1.08  & 1.09  & 1.13  & 1.25  & 0.09 \\
        0  & 0 & 1  & 4  & 1.19  & 1.44  & 1.52  & 1.52  & 1.59  & 1.83  & 0.15 \\
        0  & 0 & 1  & 8  & 1.72  & 2.07  & 2.18  & 2.19  & 2.29  & 2.70  & 0.22 \\
        0  & 0 & 1  & 16 & 2.55  & 2.99  & 3.16  & 3.17  & 3.32  & 4.05  & 0.33 \\
        \hline
        0  & 0 & 2  & 2  & 0.99  & 1.16  & 1.22  & 1.21  & 1.26  & 1.42  & 0.10 \\
        0  & 0 & 4  & 4  & 1.39  & 1.64  & 1.71  & 1.71  & 1.77  & 2.02  & 0.13 \\
        0  & 0 & 8  & 8  & 2.06  & 2.32  & 2.42  & 2.42  & 2.51  & 2.93  & 0.18 \\
        0  & 0 & 16 & 16 & 2.95  & 3.31  & 3.44  & 3.44  & 3.56  & 4.14  & 0.25 \\
        \hline
        2  & 0 & 1  & 1  &  1.23 &  1.41 &  1.48 &  1.48 &  1.53 &  1.80 & 0.12 \\
        4  & 0 & 1  & 1  &  2.32 &  3.32 &  3.58 &  3.58 &  3.86 &  4.76 & 0.54 \\
        8  & 0 & 1  & 1  &  9.47 & 11.06 & 11.41 & 11.39 & 11.75 & 12.86 & 0.69 \\
        16 & 0 & 1  & 1  & 25.16 & 27.06 & 27.41 & 27.38 & 27.74 & 28.68 & 0.68 \\
        \hline
      \end{tabular}
    \caption{Stationary White Noise}
    \end{center}
    \begin{tablenotes}
        \item{\footnotesize N = 500; T = 100}
        \item {\footnotesize \textbf{Note}: The DTW value is divided by the series length (T).}
        \item {\footnotesize \textbf{Note}: The distributions of the DTW samples are really symmetric. The median and mean remain close in value over all configurations.}
        \item{\footnotesize \textbf{Note}: For the stationary series the deterministic component (distance between the mean values) plays an influencial role in the location of the center of the DTW distribution.}
    \end{tablenotes}
\end{table}

\begin{table}[!htb]
    \begin{center}
      \begin{tabular}{| r r r r | r r r r r r r |}
        \hline
        $\omega_{A}$ & $\omega_{B}$ & $\sigma^{2}_{A}$ & $\sigma^{2}_{B}$ & Min & 1st Qu. & Median & Mean & 3rd Qu. & Max & IQR \\
        \hline
        0 & 0 & 1  & 1  & 0.84 &  2.36 &  4.42 &  7.17 &  9.31 &  46.87 &  6.94  \\
        \hline
        0 & 0 & 1  & 2  & 1.39 &  3.57 &  7.05 & 10.09 & 13.84 &  50.89 & 10.27  \\
        0 & 0 & 1  & 4  & 1.70 &  4.87 &  9.01 & 12.88 & 17.17 &  74.00 & 12.29  \\
        0 & 0 & 1  & 8  & 2.88 &  6.81 & 12.30 & 16.34 & 22.30 &  83.36 & 15.48  \\
        0 & 0 & 1  & 16 & 3.97 & 10.93 & 18.41 & 23.23 & 32.04 & 101.38 & 21.11  \\
        \hline
        0 & 0 & 2  & 2  & 1.43 &  3.44 &  6.48 & 10.41 & 13.85 &  69.20 & 10.41  \\
        0 & 0 & 4  & 4  & 1.73 &  4.83 &  8.98 & 14.41 & 19.41 &  91.95 & 14.58  \\
        0 & 0 & 8  & 8  & 2.75 &  7.38 & 15.66 & 23.12 & 31.83 & 152.90 & 24.45  \\
        0 & 0 & 16 & 16 & 4.26 & 10.05 & 19.77 & 31.23 & 40.61 & 251.30 & 30.56  \\
        \hline
      \end{tabular}
    \caption{Random Walk without Drift}
    \end{center}
    \begin{tablenotes}
        \item{\footnotesize N = 500; T = 100}
        \item {\footnotesize \textbf{Note}: The DTW value is divided by the series length (T).}
        \item{\footnotesize \textbf{Note}: }
    \end{tablenotes}
\end{table}

\begin{table}[!htb]
    \begin{center}
      \begin{tabular}{| r r r r | r r r r r r r |}
        \hline
        $\omega_{A}$ & $\omega_{B}$ & $\sigma^{2}_{A}$ & $\sigma^{2}_{B}$ & Min & 1st Qu. & Median & Mean & 3rd Qu. & Max & IQR \\
        \hline
        1  & 1 & 1  & 1  & 0.85   & 1.12   & 1.48   &  1.97  &  2.36  &  8.62  & 1.24  \\
        \hline
        1  & 1 & 1  & 2  & 1.00   & 1.42   & 2.02   &  2.75  &  3.48  & 12.82  & 2.05  \\
        1  & 1 & 1  & 4  & 1.31   & 1.97   & 2.79   &  4.01  &  4.62  & 19.91  & 2.64  \\
        1  & 1 & 1  & 8  & 1.68   & 3.08   & 4.55   &  6.49  &  8.08  & 40.11  & 5.00  \\
        1  & 1 & 1  & 16 & 2.88   & 5.19   & 7.64   & 11.02  & 13.26  & 68.12  & 8.07  \\
        \hline
        1  & 1 & 2  & 2  & 1.23   & 1.63   &  2.38  &  3.23  &  3.76  &  22.63 &  2.13  \\
        1  & 1 & 4  & 4  & 1.73   & 2.43   &  3.66  &  5.62  &  7.60  &  36.24 &  5.17  \\
        1  & 1 & 8  & 8  & 2.60   & 4.06   &  6.49  &  9.38  & 11.70  &  48.24 &  7.64  \\
        1  & 1 & 16 & 16 & 3.93   & 6.57   & 10.87  & 17.00  & 20.03  & 108.67 & 13.46  \\
        \hline
        2  & 1 & 1  & 1  &  15.18 &  27.33 &  32.43 &  32.74 &  37.32 &  59.31 &  9.99  \\
        4  & 1 & 1  & 1  & 115.79 & 136.75 & 143.33 & 143.48 & 150.31 & 181.75 & 13.56  \\
        8  & 1 & 1  & 1  & 355.60 & 382.08 & 389.51 & 389.48 & 396.75 & 422.52 & 14.67  \\
        16 & 1 & 1  & 1  & 854.08 & 883.86 & 891.20 & 891.33 & 899.22 & 927.54 & 15.36  \\
        \hline
      \end{tabular}
    \caption{Random Walk with Drift}
    \end{center}
    \begin{tablenotes}
        \item{\footnotesize N = 500; T = 100}
        \item {\footnotesize \textbf{Note}: The DTW value is divided by the series length (T).}
        \item {\footnotesize \textbf{Note}: The distributions are skewed towards larger values of the DTW metric (mean is greater than the median).}
        \item{\footnotesize \textbf{Note}: Compare the last row of each section where variance or the distance between intercepts is set to 16. For the white noise process the strongest factor contributing to the mean DTW value is the difference between the mean values. In comparison, the response to an increase in the variance of either series is muted.}
    \end{tablenotes}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{ts_distance_method.png}
    \caption{List of distance measures included in \cite{ElsingAgon2012}}
    \label{fig:ds_dist_meas_table}
\end{figure}


\pagebreak


\printbibliography

\end{document}